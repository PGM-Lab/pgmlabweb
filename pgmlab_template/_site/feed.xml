<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">PGM-Lab</title>
<subtitle type="text">Experts in machine learning and data science.</subtitle>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml" />
<link rel="alternate" type="text/html" href="http://localhost:4000" />
<updated>2019-01-18T14:54:20+01:00</updated>
<id>http://localhost:4000/</id>
<author>
  <name></name>
  <uri>http://localhost:4000/</uri>
  <email></email>
</author>


<entry>
  <title type="html"><![CDATA[How does physics connect to machine learning?]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/how-does-physics-connect-machine-learning/" />
  <id>http://localhost:4000/how-does-physics-connect-machine-learning</id>
  <published>2017-08-11T00:00:00+02:00</published>
  <updated>2017-08-11T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;I struggled to learn machine learning. I was used to variational tricks, MCMC samplers, and discreet Taylor expansions from years of physics training. Now the concepts were mixed up. The intuitive models of physical systems were replaced by abstract models of ‘data’ and amechanical patterns of cause and effect.&lt;/p&gt;

&lt;p&gt;I had to fit these fields together. Physics and machine learning are intricately connected, but it is taking me years to make the overlaps precise. This process requires representing the new with the familiar, mapping jargon from one field to another.&lt;/p&gt;

&lt;p&gt;A simple model of magnets—the Ising model—will help illustrate the rich connection between these fields. We first analyze this model with physics intuition. Then we derive the variational principle in physics and show that it recovers the same solution.&lt;/p&gt;

&lt;p&gt;We then discover how that very same variational principle in physics opens a window into machine learning. We identify Boltzmann distributions as exponential families to make the mapping transparent, and show how approximate posterior inference is scaled to massive data thanks to the variational principle.&lt;/p&gt;

&lt;p&gt;If you have a physics background, I hope you will have a better sense of machine learning and be able to read papers in the field. If you are a machine learner, I hope you will have the context to read a statistical physics paper about mean-field theory and the Ising model.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If this article is confusing, falls short of these goals, or could be improved in any way please &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email me&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/thejaan&quot;&gt;@ me&lt;/a&gt;, or &lt;a href=&quot;https://github.com/altosaar/jaan.io/blob/master/_posts/blog/2017-08-11-how-does-physics-connect-machine-learning.md&quot;&gt;submit a pull request&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-ising-model-a-physics-perspective&quot;&gt;The Ising model, a physics perspective&lt;/h3&gt;

&lt;p&gt;Consider a lattice of spins that point up or down:&lt;/p&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;
&lt;svg xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:cc=&quot;http://creativecommons.org/ns#&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot; xmlns:svg=&quot;http://www.w3.org/2000/svg&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; viewBox=&quot;0 0 555.95477 431.12668&quot; height=&quot;431.12668&quot; width=&quot;555.95477&quot; xml:space=&quot;preserve&quot; id=&quot;svg2&quot; version=&quot;1.1&quot;&gt;&lt;metadata id=&quot;metadata8&quot;&gt;&lt;rdf:RDF&gt;&lt;cc:Work rdf:about=&quot;&quot;&gt;&lt;dc:format&gt;image/svg+xml&lt;/dc:format&gt;&lt;dc:type rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;/cc:Work&gt;&lt;/rdf:RDF&gt;&lt;/metadata&gt;&lt;defs id=&quot;defs6&quot;&gt;&lt;clipPath id=&quot;clipPath14&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path12&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath24&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path22&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath206&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path204&quot; d=&quot;m 310.5266,509.9469 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath220&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path218&quot; d=&quot;m 491.0531,421.5 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath234&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path232&quot; d=&quot;m 400.5266,420.9468 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath248&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path246&quot; d=&quot;m 311.0532,421.5 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath262&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path260&quot; d=&quot;m 580.5266,420.0532 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath276&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path274&quot; d=&quot;m 581.2,509.9469 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath290&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path288&quot; d=&quot;m 400.5266,508.5 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath304&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path302&quot; d=&quot;m 491.2,509.9469 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath318&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path316&quot; d=&quot;m 310.5266,598.9469 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath332&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path330&quot; d=&quot;m 581.2,598.9469 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath346&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path344&quot; d=&quot;m 400.5266,597.5 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath360&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path358&quot; d=&quot;m 491.2,598.9469 h 11.6 v 27.05315 h -11.6 z&quot; /&gt;&lt;/clipPath&gt;&lt;/defs&gt;&lt;g clip-path=&quot;url(#clipPath14)&quot; transform=&quot;matrix(1.3333333,0,0,-1.3333333,-324.15533,911.374)&quot; id=&quot;g10&quot;&gt;&lt;g id=&quot;g16&quot; /&gt;&lt;g id=&quot;g18&quot;&gt;&lt;g clip-path=&quot;url(#clipPath24)&quot; id=&quot;g20&quot;&gt;&lt;path id=&quot;path26&quot; style=&quot;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;g transform=&quot;matrix(0,1,1,0,316.3266,456)&quot; id=&quot;g28&quot;&gt;&lt;path id=&quot;path30&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,294.8,456)&quot; id=&quot;g32&quot;&gt;&lt;path id=&quot;path34&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,249.3266,434.4734)&quot; id=&quot;g36&quot;&gt;&lt;path id=&quot;path38&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,315.8,369)&quot; id=&quot;g40&quot;&gt;&lt;path id=&quot;path42&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,456)&quot; id=&quot;g44&quot;&gt;&lt;path id=&quot;path46&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,384.8,456)&quot; id=&quot;g48&quot;&gt;&lt;path id=&quot;path50&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,339.3266,434.4734)&quot; id=&quot;g52&quot;&gt;&lt;path id=&quot;path54&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,405.8,369)&quot; id=&quot;g56&quot;&gt;&lt;path id=&quot;path58&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,456)&quot; id=&quot;g60&quot;&gt;&lt;path id=&quot;path62&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,456)&quot; id=&quot;g64&quot;&gt;&lt;path id=&quot;path66&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,429.3266,434.4734)&quot; id=&quot;g68&quot;&gt;&lt;path id=&quot;path70&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,495.8,369)&quot; id=&quot;g72&quot;&gt;&lt;path id=&quot;path74&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,586.3266,456)&quot; id=&quot;g76&quot;&gt;&lt;path id=&quot;path78&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,564.8,456)&quot; id=&quot;g80&quot;&gt;&lt;path id=&quot;path82&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,519.3265,434.4734)&quot; id=&quot;g84&quot;&gt;&lt;path id=&quot;path86&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,585.8,369)&quot; id=&quot;g88&quot;&gt;&lt;path id=&quot;path90&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,609.3265,434.4734)&quot; id=&quot;g92&quot;&gt;&lt;path id=&quot;path94&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,316.3266,545)&quot; id=&quot;g96&quot;&gt;&lt;path id=&quot;path98&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,294.8,545)&quot; id=&quot;g100&quot;&gt;&lt;path id=&quot;path102&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,249.3266,523.4734)&quot; id=&quot;g104&quot;&gt;&lt;path id=&quot;path106&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,545)&quot; id=&quot;g108&quot;&gt;&lt;path id=&quot;path110&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,384.8,545)&quot; id=&quot;g112&quot;&gt;&lt;path id=&quot;path114&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,339.3266,523.4734)&quot; id=&quot;g116&quot;&gt;&lt;path id=&quot;path118&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,545)&quot; id=&quot;g120&quot;&gt;&lt;path id=&quot;path122&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,545)&quot; id=&quot;g124&quot;&gt;&lt;path id=&quot;path126&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,429.3266,523.4734)&quot; id=&quot;g128&quot;&gt;&lt;path id=&quot;path130&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,586.3266,545)&quot; id=&quot;g132&quot;&gt;&lt;path id=&quot;path134&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,564.8,545)&quot; id=&quot;g136&quot;&gt;&lt;path id=&quot;path138&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,519.3265,523.4734)&quot; id=&quot;g140&quot;&gt;&lt;path id=&quot;path142&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,609.3265,523.4734)&quot; id=&quot;g144&quot;&gt;&lt;path id=&quot;path146&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,316.3266,634)&quot; id=&quot;g148&quot;&gt;&lt;path id=&quot;path150&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,294.8,634)&quot; id=&quot;g152&quot;&gt;&lt;path id=&quot;path154&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,249.3266,612.4734)&quot; id=&quot;g156&quot;&gt;&lt;path id=&quot;path158&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,634)&quot; id=&quot;g160&quot;&gt;&lt;path id=&quot;path162&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,384.8,634)&quot; id=&quot;g164&quot;&gt;&lt;path id=&quot;path166&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,339.3266,612.4734)&quot; id=&quot;g168&quot;&gt;&lt;path id=&quot;path170&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,634)&quot; id=&quot;g172&quot;&gt;&lt;path id=&quot;path174&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,634)&quot; id=&quot;g176&quot;&gt;&lt;path id=&quot;path178&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,429.3266,612.4734)&quot; id=&quot;g180&quot;&gt;&lt;path id=&quot;path182&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,586.3266,634)&quot; id=&quot;g184&quot;&gt;&lt;path id=&quot;path186&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,564.8,634)&quot; id=&quot;g188&quot;&gt;&lt;path id=&quot;path190&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,519.3265,612.4734)&quot; id=&quot;g192&quot;&gt;&lt;path id=&quot;path194&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,609.3265,612.4734)&quot; id=&quot;g196&quot;&gt;&lt;path id=&quot;path198&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g200&quot;&gt;&lt;g clip-path=&quot;url(#clipPath206)&quot; id=&quot;g202&quot;&gt;&lt;g transform=&quot;matrix(0,-1,-1,0,316.3266,536)&quot; id=&quot;g208&quot;&gt;&lt;path id=&quot;path210&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path212&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 311.5266,520.5469 4.8,-9.6 4.8,9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g214&quot;&gt;&lt;g clip-path=&quot;url(#clipPath220)&quot; id=&quot;g216&quot;&gt;&lt;g transform=&quot;matrix(0,-1,-1,0,496.8531,447.5532)&quot; id=&quot;g222&quot;&gt;&lt;path id=&quot;path224&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path226&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 492.0531,432.1 4.8,-9.6 4.8,9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g228&quot;&gt;&lt;g clip-path=&quot;url(#clipPath234)&quot; id=&quot;g230&quot;&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,421.9468)&quot; id=&quot;g236&quot;&gt;&lt;path id=&quot;path238&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path240&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 411.1266,437.4 -4.8,9.6 -4.8,-9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g242&quot;&gt;&lt;g clip-path=&quot;url(#clipPath248)&quot; id=&quot;g244&quot;&gt;&lt;g transform=&quot;matrix(0,1,1,0,316.8532,422.5)&quot; id=&quot;g250&quot;&gt;&lt;path id=&quot;path252&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path254&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 321.6532,437.9532 -4.8,9.6 -4.8,-9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g256&quot;&gt;&lt;g clip-path=&quot;url(#clipPath262)&quot; id=&quot;g258&quot;&gt;&lt;g transform=&quot;matrix(0,1,1,0,586.3266,421.0532)&quot; id=&quot;g264&quot;&gt;&lt;path id=&quot;path266&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path268&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 591.1266,436.5063 -4.8,9.6 -4.8,-9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g270&quot;&gt;&lt;g clip-path=&quot;url(#clipPath276)&quot; id=&quot;g272&quot;&gt;&lt;g transform=&quot;matrix(0,-1,-1,0,587,536)&quot; id=&quot;g278&quot;&gt;&lt;path id=&quot;path280&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path282&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 582.2,520.5469 4.8,-9.6 4.8,9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g284&quot;&gt;&lt;g clip-path=&quot;url(#clipPath290)&quot; id=&quot;g286&quot;&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,509.5)&quot; id=&quot;g292&quot;&gt;&lt;path id=&quot;path294&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path296&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 411.1266,524.9532 -4.8,9.6 -4.8,-9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g298&quot;&gt;&lt;g clip-path=&quot;url(#clipPath304)&quot; id=&quot;g300&quot;&gt;&lt;g transform=&quot;matrix(0,1,1,0,497,510.9469)&quot; id=&quot;g306&quot;&gt;&lt;path id=&quot;path308&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path310&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 501.8,526.4 -4.8,9.6 -4.8,-9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g312&quot;&gt;&lt;g clip-path=&quot;url(#clipPath318)&quot; id=&quot;g314&quot;&gt;&lt;g transform=&quot;matrix(0,-1,-1,0,316.3266,625)&quot; id=&quot;g320&quot;&gt;&lt;path id=&quot;path322&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path324&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 311.5266,609.5469 4.8,-9.6 4.8,9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g326&quot;&gt;&lt;g clip-path=&quot;url(#clipPath332)&quot; id=&quot;g328&quot;&gt;&lt;g transform=&quot;matrix(0,-1,-1,0,587,625)&quot; id=&quot;g334&quot;&gt;&lt;path id=&quot;path336&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path338&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 582.2,609.5469 4.8,-9.6 4.8,9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g340&quot;&gt;&lt;g clip-path=&quot;url(#clipPath346)&quot; id=&quot;g342&quot;&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,598.5)&quot; id=&quot;g348&quot;&gt;&lt;path id=&quot;path350&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path352&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 411.1266,613.9532 -4.8,9.6 -4.8,-9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g354&quot;&gt;&lt;g clip-path=&quot;url(#clipPath360)&quot; id=&quot;g356&quot;&gt;&lt;g transform=&quot;matrix(0,1,1,0,497,599.9469)&quot; id=&quot;g362&quot;&gt;&lt;path id=&quot;path364&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 0,0 h 15.45315 1&quot; /&gt;&lt;/g&gt;&lt;path id=&quot;path366&quot; style=&quot;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;m 501.8,615.4 -4.8,9.6 -4.8,-9.6 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;p&gt;What features might make this a convincing model of magnetism?&lt;/p&gt;

&lt;p&gt;Think about playing with magnets—if you put them close together, they pull each other closer. They repulse each other when their poles oppose, and if they’re far apart, they don’t attract.&lt;/p&gt;

&lt;p&gt;This means neighboring spins should affect each other in our model: if the spins around &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; point upward, it should also want to point upward.&lt;/p&gt;

&lt;p&gt;Let’s refer to the spin at location &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;. A spin can be in one of two &lt;strong&gt;states&lt;/strong&gt;: a spin can point up (&lt;script type=&quot;math/tex&quot;&gt;s_i=+1&lt;/script&gt;) or down (&lt;script type=&quot;math/tex&quot;&gt;s_i=-1&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;We can capture our intuition about spins being attracted to each other (they want to point in the same direction) or repulsed (they want to point in opposite directions) by introducing a parameter &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;. This interaction parameter captures the &lt;strong&gt;interaction strength&lt;/strong&gt; between spin &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and spin &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If two neighboring spins point in the same direction, we’ll have them contribute a term &lt;script type=&quot;math/tex&quot;&gt;-J&lt;/script&gt; to the total energy; if they point in opposing directions, they will contribute &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This lets us write the &lt;strong&gt;energy function&lt;/strong&gt;, or Hamiltonian, of the system:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(s_1, s_2,...,s_N) = -\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^NJ_{ij} s_i s_j&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;J_{ij} = J&lt;/script&gt; if spins &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; are neighbors, and &lt;script type=&quot;math/tex&quot;&gt;J_{ij} = 0&lt;/script&gt; otherwise. The factor of &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2}&lt;/script&gt; in front is to account for double counting from the sums over both &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;. Note that the system has finitely many spins (&lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; spins).&lt;/p&gt;

&lt;p&gt;A spin &lt;strong&gt;configuration&lt;/strong&gt; or state of the system is a specific setting of values for all spins. The set &lt;script type=&quot;math/tex&quot;&gt;\{s_1=+1, s_2=+1, s_3=-1, ..., s_N=+1\}&lt;/script&gt; is an example of a configuration.&lt;/p&gt;

&lt;p&gt;The second law of thermodynamics says that at a fixed temperature and entropy, a system will seek configurations that minimize its energy. This lets us reason about interactions.&lt;/p&gt;

&lt;p&gt;If the interaction strength &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; is zero, the spins do not interact and the system has the same energy, zero, for all configurations (i.e. the energy is trivially minimized). But if the interaction strength &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; is positive, the spins will tend to align to minimize the energy of the system &lt;script type=&quot;math/tex&quot;&gt;E(s_1, s_2,...,s_N)&lt;/script&gt;. This corresponds to minimization because of the minus sign convention in front of the sum in the energy function.&lt;/p&gt;

&lt;p&gt;Now let’s introduce a &lt;strong&gt;magnetic field&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;. Imagine the lattice of spins immersed in a magnetic field, perhaps the ambient field from the earth’s crust. The magnetic field affects every spin independently, and each spin will try to align with the field. We can include the magnetic field in the energy of the system by summing the independent contributions for each spin:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(s_1, s_2,...,s_N) = -\frac{1}{2}\sum_{i, j} J_{ij}s_i s_j - H\sum_i s_i&lt;/script&gt;

&lt;p&gt;We can reason about the magnetic field strength &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; by imagining what happens if it is large or small (strong or weak). If &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; is large and the interactions between spins are weak, the magnetic field term will dominate and the spins will align with the magnetic field to minimize energy. But if the magnetic field is small, it is more difficult to reason about.&lt;/p&gt;

&lt;p&gt;Now that we have defined the Ising model and its characteristics, let’s think about our goals. What questions can we answer about this Ising model? For example, if we observe the system, what state will it be in—what are the most likely spin configurations? What is the average magnetization?&lt;/p&gt;

&lt;h3 id=&quot;the-boltzmann-distribution&quot;&gt;The Boltzmann distribution&lt;/h3&gt;

&lt;p&gt;Can we make our goals more precise and make math from words? To do this, we need to define a distribution over spin configurations. It is straightforward to derive the probability of finding the system in an equilibrium state &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s_1, s_2,...,s_N) = \frac{e^{-\beta E(s_1, s_2,...,s_N)}}{Z}&lt;/script&gt;

&lt;p&gt;This is the &lt;strong&gt;Boltzmann distribution&lt;/strong&gt;. The numerator is called the Boltzmann factor for a particular configuration. This factor gives high or low weight to a specific state of the system according to the energy for that state.&lt;/p&gt;

&lt;p&gt;We query the Boltzmann distribution at a specific configuration of spins to get the probability of finding the system in this state.&lt;/p&gt;

&lt;p&gt;For example, say the first spins in our configuration happen to be up, up, down, etc. We plug this in and get  &lt;script type=&quot;math/tex&quot;&gt;p(s_1=+1, s_2=+1, s_3=-1,...,s_N=+1)=0.7321&lt;/script&gt;. This means this state was pretty likely.&lt;/p&gt;

&lt;p&gt;This distribution behaves intuitively: low energy states are more probable than configurations with high energy. For example, if &lt;script type=&quot;math/tex&quot;&gt;J=+1&lt;/script&gt;, the spins will align, and the state where all spins point in the same direction is most probable. Why? Because it leads to the most negative energy function, which corresponds to the Boltzmann factor with the largest weight.&lt;/p&gt;

&lt;p&gt;The parameter &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; is proportional to the inverse temperature, &lt;script type=&quot;math/tex&quot;&gt;\beta = \frac{1}{k_BT}&lt;/script&gt; and is used for notational convenience. (Specifically, it includes the constant &lt;script type=&quot;math/tex&quot;&gt;k_B&lt;/script&gt; to make the probability density dimensionless.) Temperature affects the model by controlling how important the interactions are. If &lt;script type=&quot;math/tex&quot;&gt;T\rightarrow \infty&lt;/script&gt; we are at a high temperature, and the inverse temperature is small with &lt;script type=&quot;math/tex&quot;&gt;\beta \ll 1&lt;/script&gt;, so the interaction strength &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; is not important. But at low temperatures, the inverse temperature is small and &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; divided by a small number is big, so interactions have a large effect on the system’s behavior.&lt;/p&gt;

&lt;h3 id=&quot;the-partition-function&quot;&gt;The partition function&lt;/h3&gt;

&lt;p&gt;The denominator &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is of utmost importance. It ensures that the distribution integrates to &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and is thus a valid probability distribution. We need this normalization to calculate properties of the system. Calculating mean values and other moments can only be done with a probability mass function. The name of &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is “&lt;strong&gt;partition function&lt;/strong&gt;” or “normalizing constant”. It is the sum of each state’s Boltzmann factor:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \sum_{s_1=\pm1}\sum_{s_2=\pm1}...\sum_{s_N=\pm1}e^{-\beta E(s_1, s_2, ..., s_N)}&lt;/script&gt;

&lt;p&gt;I explicitly wrote out the sum to illustrate why we can’t evaluate this distribution: we need to sum over all possible configurations. Each spin has two states, and there are &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; spins. This leads to &lt;script type=&quot;math/tex&quot;&gt;2^N&lt;/script&gt; terms in the sum. For a small system with a hundred spins, this is already greater than the number of atoms in the universe so we can never hope to calculate it &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;using-the-boltzmann-distribution-to-calculate-properties-of-the-system&quot;&gt;Using the Boltzmann distribution to calculate properties of the system&lt;/h3&gt;

&lt;p&gt;We arrived at a probability distribution describing which states of the system are likely, however we were stumped by the intractable partition function. Let’s temporarily assume we have infinite computation and &lt;em&gt;can&lt;/em&gt; calculate the Boltzmann distribution’s partition function. What are some interesting things we can learn about the system from it’s Boltzmann distribution?&lt;/p&gt;

&lt;p&gt;This distribution lets us to calculate properties of the system as a whole by taking &lt;strong&gt;expectations&lt;/strong&gt; (i.e. calculating observable quantities). For example, the magnetization &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is the average magnetization over all spins:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m = \frac{1}{N} \langle  s_1 + s_1 + ... + s_N \rangle = \langle  s_i \rangle&lt;/script&gt;

&lt;p&gt;Why should we care about this magnetization? It tells us about the system as a whole, the macrostate, rather than a specific microstate. We lose specificity because we can’t say anything about the first spin &lt;script type=&quot;math/tex&quot;&gt;s_1&lt;/script&gt;, but we learn about how it behaves across all possible states of the rest of the spins.&lt;/p&gt;

&lt;p&gt;If the spins are aligned, the system is in an ordered state and the magnetization has a positive or negative sign. If the spins are anti-aligned, the system is disordered and the average magnetization is zero.&lt;/p&gt;

&lt;p&gt;These are global &lt;strong&gt;phases&lt;/strong&gt; of the system, and they depend on temperature. If the temperature &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; goes to infinity, the inverse temperature &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; goes to zero, and all states of the system are equally likely, as described by the Boltzmann distribution. But if the temperature is finite, then some states are more likely than others, and the system can transition between ordered and disordered phases. Such &lt;strong&gt;phase transitions&lt;/strong&gt; and how they depend on the temperature are important for comparing how well this Ising model matches real-world materials &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Let’s remember that we can’t evaluate the partition function &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;. This situation seems hopeless for answering interesting questions like calculating the magnetization. But thankfully, we may be able to simplify the problem by considering each spin independently and figuring out an approximation…&lt;/p&gt;

&lt;h3 id=&quot;mean-field-theory-in-physics&quot;&gt;Mean-field theory in physics&lt;/h3&gt;

&lt;p&gt;Because we cannot evaluate the intractable sum required to calculate the partition function, we turn to &lt;strong&gt;mean-field theory&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This is an approximation technique that can still let us answer questions about the system such as the average magnetization. We will study the dependence of the magnetization &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; on temperature.&lt;/p&gt;

&lt;p&gt;To demonstrate the technique, it is easiest to focus on a single spin:&lt;/p&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;
&lt;svg xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:cc=&quot;http://creativecommons.org/ns#&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot; xmlns:svg=&quot;http://www.w3.org/2000/svg&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; viewBox=&quot;0 0 318.76706 312.94681&quot; height=&quot;312.94681&quot; width=&quot;318.76706&quot; xml:space=&quot;preserve&quot; id=&quot;svg2&quot; version=&quot;1.1&quot;&gt;&lt;metadata id=&quot;metadata8&quot;&gt;&lt;rdf:RDF&gt;&lt;cc:Work rdf:about=&quot;&quot;&gt;&lt;dc:format&gt;image/svg+xml&lt;/dc:format&gt;&lt;dc:type rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;/cc:Work&gt;&lt;/rdf:RDF&gt;&lt;/metadata&gt;&lt;defs id=&quot;defs6&quot;&gt;&lt;clipPath id=&quot;clipPath14&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path12&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath24&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path22&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath130&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path128&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath136&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path134&quot; d=&quot;m 448,474 h 96 v 99 h -96 z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath140&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path138&quot; d=&quot;m 448,474 h 96 v 99 h -96 z&quot; /&gt;&lt;/clipPath&gt;&lt;mask id=&quot;mask142&quot; height=&quot;1&quot; width=&quot;1&quot; y=&quot;0&quot; x=&quot;0&quot; maskUnits=&quot;userSpaceOnUse&quot;&gt;&lt;image id=&quot;image144&quot; xlink:href=&quot;data:image/png;base64,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&quot; preserveAspectRatio=&quot;none&quot; height=&quot;1&quot; width=&quot;1&quot; /&gt;&lt;/mask&gt;&lt;clipPath id=&quot;clipPath170&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path168&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath176&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path174&quot; d=&quot;M 433,458 H 561 V 589 H 433 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath180&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path178&quot; d=&quot;M 433,458 H 561 V 589 H 433 Z&quot; /&gt;&lt;/clipPath&gt;&lt;mask id=&quot;mask182&quot; height=&quot;1&quot; width=&quot;1&quot; y=&quot;0&quot; x=&quot;0&quot; maskUnits=&quot;userSpaceOnUse&quot;&gt;&lt;image id=&quot;image184&quot; xlink:href=&quot;data:image/png;base64,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&quot; preserveAspectRatio=&quot;none&quot; height=&quot;1&quot; width=&quot;1&quot; /&gt;&lt;/mask&gt;&lt;/defs&gt;&lt;g clip-path=&quot;url(#clipPath14)&quot; transform=&quot;matrix(1.3333333,0,0,-1.3333333,-503.338,851.59987)&quot; id=&quot;g10&quot;&gt;&lt;g id=&quot;g16&quot; /&gt;&lt;g id=&quot;g18&quot;&gt;&lt;g clip-path=&quot;url(#clipPath24)&quot; id=&quot;g20&quot;&gt;&lt;path id=&quot;path26&quot; style=&quot;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,456)&quot; id=&quot;g28&quot;&gt;&lt;path id=&quot;path30&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,456)&quot; id=&quot;g32&quot;&gt;&lt;path id=&quot;path34&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,384.8,545)&quot; id=&quot;g36&quot;&gt;&lt;path id=&quot;path38&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,545)&quot; id=&quot;g40&quot;&gt;&lt;path id=&quot;path42&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,545)&quot; id=&quot;g44&quot;&gt;&lt;path id=&quot;path46&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,429.3266,523.4734)&quot; id=&quot;g48&quot;&gt;&lt;path id=&quot;path50&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,564.8,545)&quot; id=&quot;g52&quot;&gt;&lt;path id=&quot;path54&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,519.3265,523.4734)&quot; id=&quot;g56&quot;&gt;&lt;path id=&quot;path58&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,634)&quot; id=&quot;g60&quot;&gt;&lt;path id=&quot;path62&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;translate(483,513)&quot; id=&quot;g64&quot;&gt;&lt;text id=&quot;text68&quot; style=&quot;font-variant:normal;font-weight:300;font-size:36px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;tspan id=&quot;tspan66&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;s&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(483,513)&quot; id=&quot;g70&quot;&gt;&lt;text id=&quot;text74&quot; style=&quot;font-variant:normal;font-weight:300;font-size:16px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;matrix(1,0,0,-1,18,-1.839966)&quot;&gt;&lt;tspan id=&quot;tspan72&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;1&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(483,602)&quot; id=&quot;g76&quot;&gt;&lt;text id=&quot;text80&quot; style=&quot;font-variant:normal;font-weight:300;font-size:36px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;tspan id=&quot;tspan78&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;s&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(483,602)&quot; id=&quot;g82&quot;&gt;&lt;text id=&quot;text86&quot; style=&quot;font-variant:normal;font-weight:300;font-size:16px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;matrix(1,0,0,-1,18,-1.839966)&quot;&gt;&lt;tspan id=&quot;tspan84&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;3&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(573,513)&quot; id=&quot;g88&quot;&gt;&lt;text id=&quot;text92&quot; style=&quot;font-variant:normal;font-weight:300;font-size:36px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;tspan id=&quot;tspan90&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;s&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(573,513)&quot; id=&quot;g94&quot;&gt;&lt;text id=&quot;text98&quot; style=&quot;font-variant:normal;font-weight:300;font-size:16px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;matrix(1,0,0,-1,18,-1.839966)&quot;&gt;&lt;tspan id=&quot;tspan96&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;2&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(392,513)&quot; id=&quot;g100&quot;&gt;&lt;text id=&quot;text104&quot; style=&quot;font-variant:normal;font-weight:300;font-size:36px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;tspan id=&quot;tspan102&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;s&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(392,513)&quot; id=&quot;g106&quot;&gt;&lt;text id=&quot;text110&quot; style=&quot;font-variant:normal;font-weight:300;font-size:16px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;matrix(1,0,0,-1,18,-1.839966)&quot;&gt;&lt;tspan id=&quot;tspan108&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;4&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(483,424)&quot; id=&quot;g112&quot;&gt;&lt;text id=&quot;text116&quot; style=&quot;font-variant:normal;font-weight:300;font-size:36px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;tspan id=&quot;tspan114&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;s&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(483,424)&quot; id=&quot;g118&quot;&gt;&lt;text id=&quot;text122&quot; style=&quot;font-variant:normal;font-weight:300;font-size:16px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;matrix(1,0,0,-1,18,-1.839966)&quot;&gt;&lt;tspan id=&quot;tspan120&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;5&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g124&quot;&gt;&lt;g clip-path=&quot;url(#clipPath130)&quot; id=&quot;g126&quot;&gt;&lt;g id=&quot;g132&quot; /&gt;&lt;g id=&quot;g162&quot;&gt;&lt;g style=&quot;opacity:0.70679997&quot; id=&quot;g160&quot; clip-path=&quot;url(#clipPath136)&quot;&gt;&lt;g id=&quot;g146&quot; /&gt;&lt;g id=&quot;g158&quot;&gt;&lt;g id=&quot;g156&quot; clip-path=&quot;url(#clipPath140)&quot;&gt;&lt;g id=&quot;g150&quot; transform=&quot;matrix(93.30093,0,0,93.30093,449.6761,474.823)&quot;&gt;&lt;image id=&quot;image148&quot; mask=&quot;url(#mask142)&quot; xlink:href=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAF0AAABdCAYAAADHcWrDAAAABHNCSVQICAgIfAhkiAAAAONJREFUeJzt0EERACAMwLCBf8/ggj5IFPS6ZuYMT+064EemB0wPmB4wPWB6wPSA6QHTA6YHTA+YHjA9YHrA9IDpAdMDpgdMD5geMD1gesD0gOkB0wOmB0wPmB4wPWB6wPSA6QHTA6YHTA+YHjA9YHrA9IDpAdMDpgdMD5geMD1gesD0gOkB0wOmB0wPmB4wPWB6wPSA6QHTA6YHTA+YHjA9YHrA9IDpAdMDpgdMD5geMD1gesD0gOkB0wOmB0wPmB4wPWB6wPSA6QHTA6YHTA+YHjA9YHrA9IDpAdMDpgdMD5geuMGpAbkEyR5aAAAAAElFTkSuQmCC&quot; transform=&quot;matrix(1,0,0,-1,0,1)&quot; preserveAspectRatio=&quot;none&quot; height=&quot;1&quot; width=&quot;1&quot; /&gt;&lt;/g&gt;&lt;g id=&quot;g154&quot; transform=&quot;matrix(1,0,0,-1,453.1761,566.6239)&quot;&gt;&lt;path id=&quot;path152&quot; style=&quot;fill:none;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:1, 1;stroke-dashoffset:0;stroke-opacity:1&quot; d=&quot;m 73.66245,12.63848 c 16.85131,16.8513 16.85131,44.17267 0,61.02397 -16.8513,16.85131 -44.17267,16.85131 -61.02397,0 -16.851306,-16.8513 -16.851306,-44.17267 0,-61.02397 16.8513,-16.851306 44.17267,-16.851306 61.02397,0 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g id=&quot;g164&quot;&gt;&lt;g clip-path=&quot;url(#clipPath170)&quot; id=&quot;g166&quot;&gt;&lt;g id=&quot;g172&quot; /&gt;&lt;g id=&quot;g202&quot;&gt;&lt;g style=&quot;opacity:0.66519997&quot; id=&quot;g200&quot; clip-path=&quot;url(#clipPath176)&quot;&gt;&lt;g id=&quot;g186&quot; /&gt;&lt;g id=&quot;g198&quot;&gt;&lt;g id=&quot;g196&quot; clip-path=&quot;url(#clipPath180)&quot;&gt;&lt;g id=&quot;g190&quot; transform=&quot;matrix(124.755,0,0,124.755,434.4756,459.0959)&quot;&gt;&lt;image id=&quot;image188&quot; mask=&quot;url(#mask182)&quot; xlink:href=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAH0AAAB9CAYAAACPgGwlAAAABHNCSVQICAgIfAhkiAAAATdJREFUeJzt0UENACAQwLAD/57BBTzWKliyNTNnSNm/A3jP9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPMj3I9CDTg0wPuiT1AfnKKjQ8AAAAAElFTkSuQmCC&quot; transform=&quot;matrix(1,0,0,-1,0,1)&quot; preserveAspectRatio=&quot;none&quot; height=&quot;1&quot; width=&quot;1&quot; /&gt;&lt;/g&gt;&lt;g id=&quot;g194&quot; transform=&quot;matrix(1,0,0,-1,437.9756,582.3509)&quot;&gt;&lt;path id=&quot;path192&quot; style=&quot;fill:none;stroke:#000000;stroke-width:1;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:6, 6;stroke-dashoffset:0;stroke-opacity:1&quot; d=&quot;m 100.5102,17.24482 c 22.9931,22.9931 22.9931,60.27228 0,83.26538 -22.9931,22.9931 -60.27228,22.9931 -83.26538,0 -22.993094,-22.9931 -22.993094,-60.27228 0,-83.26538 22.9931,-22.993094 60.27228,-22.993094 83.26538,0 z&quot; /&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;g transform=&quot;translate(533,554)&quot; id=&quot;g204&quot;&gt;&lt;text id=&quot;text208&quot; style=&quot;font-variant:normal;font-weight:300;font-size:36px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;tspan id=&quot;tspan206&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;H&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;
&lt;figcaption&gt;The first spin of the Ising model in a magnetic field H. The magnetic field is shown with dashed lines. Its nearest neighbors provide an effective field through the interactions, denoted by lines connecting the spins.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;p&gt;The contribution of this single spin to the total energy of the system is simply the corresponding term in the energy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{s_1} = -s_1\left(J\sum_{j=2}^z+1 s_j + H\right)&lt;/script&gt;

&lt;p&gt;The sum is over the &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; nearest neighbors. For the two-dimensional lattice we are considering, &lt;script type=&quot;math/tex&quot;&gt;z = 4&lt;/script&gt;. We can rewrite this energy for a single spin in terms of the fluctuations of a spin &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt; around its mean value &lt;script type=&quot;math/tex&quot;&gt;m = \langle  s_j \rangle&lt;/script&gt;. Replacing &lt;script type=&quot;math/tex&quot;&gt;s_j = m + (s_j - m)&lt;/script&gt; gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{s_1}= -s_1(zJm + H) -J s_1 \sum_{j=2}^z+1 (s_j - m)&lt;/script&gt;

&lt;p&gt;The next step is crucial: we will ignore the fluctuations of neighboring spins around their mean value. In other words, we assume that the term &lt;script type=&quot;math/tex&quot;&gt;(s_j - m) \rightarrow 0&lt;/script&gt;, so that each of the neighbors of &lt;script type=&quot;math/tex&quot;&gt;s_1&lt;/script&gt; is simply equal to its mean value, &lt;script type=&quot;math/tex&quot;&gt;s_j = m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;When is this true?&lt;/p&gt;

&lt;p&gt;When the fluctuations around the mean value are small, such as at low temperature ‘ordered’ phases. This assumption greatly simplifies the Hamiltonian for the spin:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{s_1}^{MF} = -s_1 (zJm + H).&lt;/script&gt;

&lt;p&gt;This is the mean-field energy function for a single spin. It is equivalent to a non-interacting spin in an &lt;em&gt;effective&lt;/em&gt; magnetic field, &lt;script type=&quot;math/tex&quot;&gt;H^{eff}=zJm + H&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Why do we say this spin is non-interacting? The energy function for the spin only depends on its state, &lt;script type=&quot;math/tex&quot;&gt;s_1&lt;/script&gt;, and does not depend on the state of any other spins. We have approximated the interaction effects by the average magnetic field induced by the neighboring spins; this is the &lt;strong&gt;mean field&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In this mean-field model, each spin feels the effects of the magnetic field applied to the entire system, &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;, as well as the ‘effective’ mean field from its neighboring spins &lt;script type=&quot;math/tex&quot;&gt;zJm&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We clarify this interpretation by writing&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H\leftarrow H + \Delta H&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\Delta H = zJm&lt;/script&gt; is the the average magnetic field (‘mean field’) of the neighbors of each spin.&lt;/p&gt;

&lt;p&gt;By ignoring the fluctuations of each spin, we have reduced the complexity of the problem. Instead of &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; interacting spins, we have &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; &lt;em&gt;independent&lt;/em&gt; spins in a uniform magnetic field &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; with a small correction &lt;script type=&quot;math/tex&quot;&gt;\Delta H&lt;/script&gt; to account for the effects of interactions.&lt;/p&gt;

&lt;p&gt;We write the energy function for the mean-field model as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{MF}(s_1, s_2,...,s_{N}) = -(H+\Delta H)\sum_{i=1}^Ns_i.&lt;/script&gt;

&lt;p&gt;This shows that there are no interaction terms anymore (the term &lt;script type=&quot;math/tex&quot;&gt;s_is_j&lt;/script&gt; doesn’t occur in the energy).&lt;/p&gt;

&lt;p&gt;In other words, we can treat each spin independently, then combine the results appropriately to model the entire system!&lt;/p&gt;

&lt;p&gt;We have radically changed the nature of the problem.&lt;/p&gt;

&lt;p&gt;Instead of computing the partition function &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; for the whole system, we can now compute it &lt;em&gt;for a single spin.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is straightforward and has an analytic solution &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_{s_1} = \sum_{s_i = \pm 1} e^{-\beta s_i(H+\Delta H)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow Z_{s_1} = 2\cosh{[\beta(H+\Delta H)]}&lt;/script&gt;

&lt;p&gt;The partition function for the entire mean-field model with &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; spins is then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_{MF} = (2\cosh{[\beta(H+\Delta H)]} )^N.&lt;/script&gt;

&lt;p&gt;With the partition function in hand, we can get the Boltzmann distribution and answer questions about the system such as magnetization.&lt;/p&gt;

&lt;p&gt;We get the magnetization by taking an expectation over the distribution for the spin. The last step is to require that for any spin &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, its average magnetization should equal the magnetization of the system as a whole:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m = \sum_{s_i=\pm 1} p(s_i) s_i&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow m =\tanh{[\beta({H + \Delta H})]}&lt;/script&gt;

&lt;p&gt;This gives us a clean equation for the magnetization,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m = \tanh{[\beta(H + zJm)]},&lt;/script&gt;

&lt;p&gt;where we used that the mean-field parameter is &lt;script type=&quot;math/tex&quot;&gt;\Delta H = zJm&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This is a formula for the magnetization &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; as a function of temperature. It has no closed form solution, but we can plot both sides of the equation and see where they intersect to find the implicit solutions (drag the slider to re-plot at a new temperature):&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;800&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;/files/ising_model_magnetization.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;First let’s think about the case when there is no external field, &lt;script type=&quot;math/tex&quot;&gt;H = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For high temperatures, the equation only has one solution: &lt;script type=&quot;math/tex&quot;&gt;m = 0&lt;/script&gt;. This aligns with our intuition—if we look at the energy of the system, the inverse temperature &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; goes to zero and all states of the spins are equally likely. They average out to zero.&lt;/p&gt;

&lt;p&gt;For low temperatures, we see three solutions: &lt;script type=&quot;math/tex&quot;&gt;m=0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;m= \pm \lvert m\lvert&lt;/script&gt;. The additional &lt;script type=&quot;math/tex&quot;&gt;\pm&lt;/script&gt; solutions appear when the slope of the &lt;script type=&quot;math/tex&quot;&gt;\tanh&lt;/script&gt; function at the origin is greater than one:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dm} \tanh{[\beta zJm]} |_{m=0} &gt; 1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow  \beta zJ &gt; 1&lt;/script&gt;

&lt;p&gt;The “critical temperature” at which the phase transition occurs is when &lt;script type=&quot;math/tex&quot;&gt;\beta zJ = \frac{1}{k_B T} zJ = 1&lt;/script&gt;, or when &lt;script type=&quot;math/tex&quot;&gt;k_B T_c = zJ&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This gives us a testable prediction: we can take a magnetic material, and measure what temperature its phase transition occurs at.&lt;/p&gt;

&lt;p&gt;Have we accomplished our goal?&lt;/p&gt;

&lt;p&gt;We set out to understand the behavior of this model at various temperatures, in terms of global properties like the magnetization.&lt;/p&gt;

&lt;p&gt;By considering a single spin and approximating the effects of other spins as an effective magnetic field, we were able to reduce the complexity of the problem. This allowed us to study phase transitions. However, our exposition felt a little hand-wavy, so let’s dive into a rigorous foundation to justify our intuitions.&lt;/p&gt;

&lt;h3 id=&quot;deriving-the-variational-free-energy-principle-the-gibbs-bogoliubov-feynman-inequality&quot;&gt;Deriving the variational free energy principle: the Gibbs-Bogoliubov-Feynman inequality&lt;/h3&gt;

&lt;p&gt;Can we learn what tradeoffs we make when we make the assumption of ‘ignoring fluctuations’ of spins around their mean values? Specifically, how can we gauge the quality of results derived from our mean-field theory?&lt;/p&gt;

&lt;p&gt;We can rederive the mean-field results in the previous section by directly attacking the problem of the intractable partition function. We can try to approximate this partition function with a simpler one.&lt;/p&gt;

&lt;p&gt;Recall that the partition function &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; for the system is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \sum_{s_1, s_2, ..., s_N}e^{-\beta E(s_1, s_2,...,s_N)}&lt;/script&gt;

&lt;p&gt;where as before, the energy for the system is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(s_1, s_2,...,s_N) = -\frac{1}{2}\sum_{i, j} J_{ij}s_i s_j - H\sum_i s_i.&lt;/script&gt;

&lt;p&gt;The complexity of computing the partition function comes from the interaction term with &lt;script type=&quot;math/tex&quot;&gt;s_is_j&lt;/script&gt;. We saw that without this term, we were able to reduce the problem to dealing with a system of independent spins.&lt;/p&gt;

&lt;p&gt;To derive the variational principle, we will therefore assume an energy function of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{MF}(s_1, s_2,...,s_N) = -(H + \Delta H) \sum_{i=1}^N s_i&lt;/script&gt;

&lt;p&gt;Previously we saw that the mean-field parameter is &lt;script type=&quot;math/tex&quot;&gt;\Delta H = zJm&lt;/script&gt; which we derived using our physics intuition.&lt;/p&gt;

&lt;p&gt;Now we ask the question: is this the optimal effective magnetic field? We can think of &lt;script type=&quot;math/tex&quot;&gt;\Delta H&lt;/script&gt; as a parameter of the mean-field model that we can tune to get the best answers for the original system.&lt;/p&gt;

&lt;p&gt;This is known as &lt;strong&gt;perturbation theory&lt;/strong&gt;: we are perturbing the magnetic field of the system and trying to find the optimal perturbation that yields a good approximation to the original system.&lt;/p&gt;

&lt;p&gt;What does a ‘good approximation’ entail? Our difficulties were in computing the partition function. We therefore want to approximate the partition function of the original system &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; with the partition function of our mean-field system &lt;script type=&quot;math/tex&quot;&gt;Z_{MF}&lt;/script&gt;. Let’s hope that &lt;script type=&quot;math/tex&quot;&gt;Z_{MF}&lt;/script&gt; is easy to calculate and does not require a sum on the order of the number of atoms in the universe.&lt;/p&gt;

&lt;p&gt;First let’s see if we can express the partition function of the original system &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; in terms of our approximation. We can measure how the energy of the mean-field system deviates from the reference system by computing the fluctuations in energy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta E(s_1, s_2,...,s_N) = E(s_1, s_2,...,s_N) - E_{MF}(s_1, s_2,...,s_N)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow E = E_{MF} + \Delta E&lt;/script&gt;

&lt;p&gt;This lets us reëxpress the original partition function as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \sum_{s_1, s_2,...,s_N} \exp{[-\beta(E_{MF} + \Delta E)]}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow Z =Z_{MF} \sum_{s_1, s_2,...,s_N}\frac{\exp{(-\beta E_{MF})} \exp{(-\beta\Delta E)}}{Z_{MF}}&lt;/script&gt;

&lt;p&gt;For the next step, we need the definition of an expectation of a function &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; with respect to the mean-field Boltzmann distribution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle  A \rangle_{MF} =\sum_{s_1, s_2,...,s_N} \frac{A e^{-\beta E_{MF}}}{Z_{MF}}&lt;/script&gt;

&lt;p&gt;This means we can write the partition function of the system in terms of the mean-field partition function as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=Z_{MF}\langle \exp{(-\beta \Delta E)}\rangle_{MF}&lt;/script&gt;

&lt;p&gt;This is an exact factorization of the partition function of the original system. It is the mean-field partition function weighted by the expected Boltzmann factor for energy fluctuations away from the reference system.&lt;/p&gt;

&lt;p&gt;However, integrating this complicated exponential function is difficult, even with respect to the mean-field system. We’ll simplify it with a classic physics trick—by pulling a Taylor expansion.&lt;/p&gt;

&lt;p&gt;Let’s assume that the fluctuations of the energy are small; &lt;script type=&quot;math/tex&quot;&gt;\Delta E \ll 1&lt;/script&gt;. Then we can Taylor expand the exponent:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle \exp{(-\beta \Delta E)}\rangle_{MF}~\approx~\langle  1 - \beta \Delta E + ... \rangle_{MF}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=~1 - \beta \langle \Delta E\rangle_{MF}+...&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\exp{(-\beta \langle \Delta E\rangle_{MF})} + ...&lt;/script&gt;

&lt;p&gt;We have neglected terms of second order in the fluctuations &lt;script type=&quot;math/tex&quot;&gt;\Delta E&lt;/script&gt;. This gives us our first-order perturbation theory result for the partition function of the original system:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z \approx Z_{MF}\exp{(-\beta \langle \Delta E\rangle_{MF})}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow Z \approx Z_{MF}\exp{(-\beta \langle  E - E_{MF}\rangle_{MF})}&lt;/script&gt;

&lt;p&gt;How good is the approximation? We need a simple identity &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;: &lt;script type=&quot;math/tex&quot;&gt;e^x \geq x + 1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Let’s apply this to the expectation in the exact factorization of the partition function, taking &lt;script type=&quot;math/tex&quot;&gt;f = -\beta \Delta E&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle  e^f\rangle = e^{\langle  f\rangle} \langle  e^{(f - \langle  f \rangle)} \rangle&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\geq e^{ \langle  f \rangle} \langle 1 + f - \langle  f \rangle\rangle = e^{\langle  f \rangle}&lt;/script&gt;

&lt;p&gt;Now we can get a lower bound on the partition function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = Z_{MF}\langle  \exp{(-\beta \Delta E)}\rangle_{MF}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow Z \geq Z_{MF} \exp{[-\beta \langle  E - E_{MF}\rangle_{MF}]}&lt;/script&gt;

&lt;p&gt;This inequality is the &lt;strong&gt;Gibbs-Bogoliubov-Feynman inequality&lt;/strong&gt;. It tells us that with our mean-field approximation, we get a lower bound on the original partition function.&lt;/p&gt;

&lt;h3 id=&quot;variational-treatment-of-the-ising-model-using-the-gibbs-bogoliubov-feynman-inequality&quot;&gt;Variational treatment of the Ising model using the Gibbs-Bogoliubov-Feynman inequality&lt;/h3&gt;

&lt;p&gt;Let’s apply this theory: do we recover the same results for magnetization in the Ising model?&lt;/p&gt;

&lt;p&gt;In the mean-field Ising model, we treat each spin independently, so the energy function of the system decomposes into independent parts:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{MF}(s_1, s_2,...,s_N) = -(H + \Delta H) \sum_{i=1}^N s_i&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;\Delta H&lt;/script&gt; is the effective magnetic field strength. It is a parameter we can tune to maximize the lower bound on the partition function.&lt;/p&gt;

&lt;p&gt;Let’s plug this into the lower bound on the partition function from the Gibbs-Bogoliubov-Feynman inequality. Then we take the derivative to maximize the lower bound:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 = \frac{\partial}{\partial{\Delta H}} Z_{MF} \exp{[-\beta \langle  E - E_{MF}\rangle_{MF}]}&lt;/script&gt;

&lt;p&gt;First, we need  to evaluate the expectation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle E - E_{MF}\rangle_{MF} = -N(\frac{1}{2} Jz\langle s_1\rangle^2_{MF} - \Delta H \langle  s_1 \rangle_{MF}),&lt;/script&gt;

&lt;p&gt;where we used the mean-field assumption that the spins are independent, hence &lt;script type=&quot;math/tex&quot;&gt;\langle s_i s_j\rangle_{MF} = \langle s_i\rangle_{MF} \langle s_j\rangle_{MF}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We also assumed that for a large enough system, spins at the edges of the model (boundary conditions) can be ignored, so all spins have the same average magnetization: &lt;script type=&quot;math/tex&quot;&gt;\langle s_i\rangle_{MF}\langle s_j\rangle_{MF} = \langle s_1\rangle_{MF}^2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Plugging this in to the lower bound on the partition function and differentiating gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 = \tanh[\beta(H + \Delta H)] - \langle s_1\rangle_{MF} - Jz\langle s_1\rangle_{MF} \frac{\partial}{\partial \Delta H} \langle s_1\rangle_{MF} + \Delta H \frac{\partial}{\partial \Delta H} \langle s_1\rangle_{MF}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow \Delta H = Jz \langle s_1\rangle_{MF}.&lt;/script&gt;

&lt;p&gt;We used that &lt;script type=&quot;math/tex&quot;&gt;m = \langle s_1\rangle_{MF} =  \tanh{[\beta({H + \Delta H})]}&lt;/script&gt; from before.&lt;/p&gt;

&lt;p&gt;This confirms our earlier reasoning, that the optimal mean-field parameter is &lt;script type=&quot;math/tex&quot;&gt;\Delta H = Jzm&lt;/script&gt;. There were three steps to this process. We started by defining the model we cared about, we wrote down a mean-field approximation to it, and we maximized a lower bound on the partition function.&lt;/p&gt;

&lt;h3 id=&quot;the-machine-learning-perspective-on-the-ising-model&quot;&gt;The machine learning perspective on the Ising model&lt;/h3&gt;

&lt;p&gt;Now let’s frame what we just did in the language of machine learning. More specifically, let’s think in terms of probabilistic modeling.&lt;/p&gt;

&lt;p&gt;We need some definitions to see how the variational principle is equivalent to variational inference in machine learning.&lt;/p&gt;

&lt;p&gt;The Ising model is an &lt;strong&gt;undirected graphical model&lt;/strong&gt; or Markov random field. We can represent the conditional dependencies of the model using a graph; the nodes in the graph are random variables. These random variables are the spins of the Ising model, so two nodes are connected by an edge if they interact. This lets us encode the joint distribution of the random variables in the following diagram:&lt;/p&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;
&lt;svg xmlns:dc=&quot;http://purl.org/dc/elements/1.1/&quot; xmlns:cc=&quot;http://creativecommons.org/ns#&quot; xmlns:rdf=&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot; xmlns:svg=&quot;http://www.w3.org/2000/svg&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot; viewBox=&quot;0 0 555.10132 433.25266&quot; height=&quot;433.25266&quot; width=&quot;555.10132&quot; xml:space=&quot;preserve&quot; id=&quot;svg2&quot; version=&quot;1.1&quot;&gt;&lt;metadata id=&quot;metadata8&quot;&gt;&lt;rdf:RDF&gt;&lt;cc:Work rdf:about=&quot;&quot;&gt;&lt;dc:format&gt;image/svg+xml&lt;/dc:format&gt;&lt;dc:type rdf:resource=&quot;http://purl.org/dc/dcmitype/StillImage&quot; /&gt;&lt;/cc:Work&gt;&lt;/rdf:RDF&gt;&lt;/metadata&gt;&lt;defs id=&quot;defs6&quot;&gt;&lt;clipPath id=&quot;clipPath14&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path12&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;clipPath id=&quot;clipPath24&quot; clipPathUnits=&quot;userSpaceOnUse&quot;&gt;&lt;path id=&quot;path22&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;/clipPath&gt;&lt;/defs&gt;&lt;g clip-path=&quot;url(#clipPath14)&quot; transform=&quot;matrix(1.3333333,0,0,-1.3333333,-326.63867,913.9972)&quot; id=&quot;g10&quot;&gt;&lt;g id=&quot;g16&quot; /&gt;&lt;g id=&quot;g18&quot;&gt;&lt;g clip-path=&quot;url(#clipPath24)&quot; id=&quot;g20&quot;&gt;&lt;path id=&quot;path26&quot; style=&quot;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; d=&quot;M 0,0 H 1024 V 768 H 0 Z&quot; /&gt;&lt;g transform=&quot;matrix(0,1,1,0,316.3266,456)&quot; id=&quot;g28&quot;&gt;&lt;path id=&quot;path30&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,294.8,456)&quot; id=&quot;g32&quot;&gt;&lt;path id=&quot;path34&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,249.3266,434.4734)&quot; id=&quot;g36&quot;&gt;&lt;path id=&quot;path38&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,315.8,369)&quot; id=&quot;g40&quot;&gt;&lt;path id=&quot;path42&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,456)&quot; id=&quot;g44&quot;&gt;&lt;path id=&quot;path46&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,384.8,456)&quot; id=&quot;g48&quot;&gt;&lt;path id=&quot;path50&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,339.3266,434.4734)&quot; id=&quot;g52&quot;&gt;&lt;path id=&quot;path54&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,405.8,369)&quot; id=&quot;g56&quot;&gt;&lt;path id=&quot;path58&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,456)&quot; id=&quot;g60&quot;&gt;&lt;path id=&quot;path62&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,456)&quot; id=&quot;g64&quot;&gt;&lt;path id=&quot;path66&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,429.3266,434.4734)&quot; id=&quot;g68&quot;&gt;&lt;path id=&quot;path70&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,495.8,369)&quot; id=&quot;g72&quot;&gt;&lt;path id=&quot;path74&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,586.3266,456)&quot; id=&quot;g76&quot;&gt;&lt;path id=&quot;path78&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,564.8,456)&quot; id=&quot;g80&quot;&gt;&lt;path id=&quot;path82&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,519.3265,434.4734)&quot; id=&quot;g84&quot;&gt;&lt;path id=&quot;path86&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,585.8,369)&quot; id=&quot;g88&quot;&gt;&lt;path id=&quot;path90&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,609.3265,434.4734)&quot; id=&quot;g92&quot;&gt;&lt;path id=&quot;path94&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,316.3266,545)&quot; id=&quot;g96&quot;&gt;&lt;path id=&quot;path98&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,294.8,545)&quot; id=&quot;g100&quot;&gt;&lt;path id=&quot;path102&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,249.3266,523.4734)&quot; id=&quot;g104&quot;&gt;&lt;path id=&quot;path106&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,545)&quot; id=&quot;g108&quot;&gt;&lt;path id=&quot;path110&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,384.8,545)&quot; id=&quot;g112&quot;&gt;&lt;path id=&quot;path114&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,339.3266,523.4734)&quot; id=&quot;g116&quot;&gt;&lt;path id=&quot;path118&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,545)&quot; id=&quot;g120&quot;&gt;&lt;path id=&quot;path122&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,545)&quot; id=&quot;g124&quot;&gt;&lt;path id=&quot;path126&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,429.3266,523.4734)&quot; id=&quot;g128&quot;&gt;&lt;path id=&quot;path130&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,586.3266,545)&quot; id=&quot;g132&quot;&gt;&lt;path id=&quot;path134&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,564.8,545)&quot; id=&quot;g136&quot;&gt;&lt;path id=&quot;path138&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,519.3265,523.4734)&quot; id=&quot;g140&quot;&gt;&lt;path id=&quot;path142&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,609.3265,523.4734)&quot; id=&quot;g144&quot;&gt;&lt;path id=&quot;path146&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,316.3266,634)&quot; id=&quot;g148&quot;&gt;&lt;path id=&quot;path150&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,294.8,634)&quot; id=&quot;g152&quot;&gt;&lt;path id=&quot;path154&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,249.3266,612.4734)&quot; id=&quot;g156&quot;&gt;&lt;path id=&quot;path158&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,406.3266,634)&quot; id=&quot;g160&quot;&gt;&lt;path id=&quot;path162&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,384.8,634)&quot; id=&quot;g164&quot;&gt;&lt;path id=&quot;path166&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,339.3266,612.4734)&quot; id=&quot;g168&quot;&gt;&lt;path id=&quot;path170&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,496.3266,634)&quot; id=&quot;g172&quot;&gt;&lt;path id=&quot;path174&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,474.8,634)&quot; id=&quot;g176&quot;&gt;&lt;path id=&quot;path178&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,429.3266,612.4734)&quot; id=&quot;g180&quot;&gt;&lt;path id=&quot;path182&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(0,1,1,0,586.3266,634)&quot; id=&quot;g184&quot;&gt;&lt;path id=&quot;path186&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,564.8,634)&quot; id=&quot;g188&quot;&gt;&lt;path id=&quot;path190&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;m 36.74816,6.304987 c 8.40665,8.406653 8.40665,22.036523 0,30.443173 -8.40665,8.40665 -22.03652,8.40665 -30.443173,0 -8.406649,-8.40665 -8.406649,-22.03652 0,-30.443173 8.406653,-8.406649 22.036523,-8.406649 30.443173,0 z&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,519.3265,612.4734)&quot; id=&quot;g192&quot;&gt;&lt;path id=&quot;path194&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;matrix(1,0,0,-1,609.3265,612.4734)&quot; id=&quot;g196&quot;&gt;&lt;path id=&quot;path198&quot; style=&quot;fill:none;stroke:#000000;stroke-width:2;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1&quot; d=&quot;M 0,0 H 45.05315&quot; /&gt;&lt;/g&gt;&lt;g transform=&quot;translate(486,513)&quot; id=&quot;g200&quot;&gt;&lt;text id=&quot;text204&quot; style=&quot;font-variant:normal;font-weight:300;font-size:36px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;scale(1,-1)&quot;&gt;&lt;tspan id=&quot;tspan202&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;s&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;g transform=&quot;translate(486,513)&quot; id=&quot;g206&quot;&gt;&lt;text id=&quot;text210&quot; style=&quot;font-variant:normal;font-weight:300;font-size:16px;font-family:Helvetica;-inkscape-font-specification:Helvetica-Light;writing-mode:lr-tb;fill:#000000;fill-opacity:1;fill-rule:nonzero;stroke:none&quot; transform=&quot;matrix(1,0,0,-1,18,-1.839966)&quot;&gt;&lt;tspan id=&quot;tspan208&quot; y=&quot;0&quot; x=&quot;0&quot;&gt;i&lt;/tspan&gt;&lt;/text&gt;
&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;

&lt;figcaption&gt;
A representation of the Ising model as an undirected graphical model. The nodes are random variables (spins) and edges denote conditional dependencies between their distributions.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;p&gt;The Boltzmann distribution is a parameterization of the joint distribution of this graphical model. This figure looks very similar to the physics spin-based representation—the spins are random variables. We can also write the joint distribution of the nodes in exponential family form. Exponential family distributions let us reason about a broad class of models and deserve a header.&lt;/p&gt;

&lt;h3 id=&quot;exponential-families&quot;&gt;Exponential families&lt;/h3&gt;

&lt;p&gt;A way to parameterize probability distributions like the Ising model is with &lt;strong&gt;exponential families&lt;/strong&gt;. These are families of distributions that support a representation in this specific, convenient mathematical form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x ; \eta) = h(x)e^{\eta^\top t(x) - a(\eta)}&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; is called the natural parameter, &lt;script type=&quot;math/tex&quot;&gt;h(x)&lt;/script&gt; is the base measure, &lt;script type=&quot;math/tex&quot;&gt;t(x)&lt;/script&gt; the sufficient statistic and &lt;script type=&quot;math/tex&quot;&gt;a(\eta)&lt;/script&gt; is the log normalizer, or log partition function. I was confused about exponential families for a long time and found concrete derivation helpful.&lt;/p&gt;

&lt;p&gt;For example, we are used to seeing the Bernoulli distribution in the following form &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x ; \pi) = \pi^x(1-\pi)^{(1-x)}&lt;/script&gt;

&lt;p&gt;We can rewrite this in exponential family form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x; \eta) = \exp{\{x\log \pi + (1-x)\log{(1-\pi)}\}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow p(x; \eta)=\exp{\{x\log \frac{\pi}{1-\pi} + \log{(1-\pi)}\}}&lt;/script&gt;

&lt;p&gt;Comparing to the above formula for exponential families reveals the natural parameter, base measure, sufficient statistic, and log normalizer for the Bernoulli, given by &lt;script type=&quot;math/tex&quot;&gt;\eta = \log{\frac{\pi}{1-\pi}}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;t(x) = x&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a(\eta) = -\log{(1-\pi)} = \log{(1+e^\eta)}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;h(x) = 1&lt;/script&gt; respectively.&lt;/p&gt;

&lt;p&gt;More connections to physics: the log normalizer is the log of the partition function. This is made clear in the exponential family form of the Bernoulli: &lt;script type=&quot;math/tex&quot;&gt;\log Z = \log \sum_{x\in\{0,1\}} e^{\eta x} = \log{(1+e^\eta)}&lt;/script&gt;. We can now identify the parameter &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; as a analogous to temperature, with &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; as a spin. We’ve identified the Ising model’s exponential family form!&lt;/p&gt;

&lt;h3 id=&quot;the-exponential-family-form-of-the-ising-model&quot;&gt;The exponential family form of the Ising model&lt;/h3&gt;

&lt;p&gt;Let’s connect this to the energy function of the Ising model by writing its Boltzmann distribution in exponential family form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s_1, s_2,...,s_N; \beta, J, H) = \frac{e^{-\beta E(s_1, ..., s_N)}}{Z}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s_1, s_2,...,s_N; \beta, J, H) = \exp{\{-\sum_{(i, j)\in E}\beta Js_is_j + -\sum_{i \in V}\beta Hs_i  - \log{Z}\}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s_1, s_2,...,s_N; \theta)=\exp{\{ -\sum_{(i, j)\in E} \theta_{ij}s_is_j -\sum_{i \in V} \theta_i s_i - a(\theta)\}}&lt;/script&gt;

&lt;p&gt;We have introduced some new notation common to graphical models: we have specified a joint distribution over a collection of random variables &lt;script type=&quot;math/tex&quot;&gt;\{s_1, ..., s_N\}&lt;/script&gt; that live on the graph over vertices &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;, joined by edges in the set &lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This is the exponential family form of the Ising model, a &lt;strong&gt;probability model&lt;/strong&gt; with model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. To equate it to the form we saw earlier, set &lt;script type=&quot;math/tex&quot;&gt;\theta_{ij} = \frac{1}{2}\beta J&lt;/script&gt; if &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; share an edge (i.e. they are neighbors), and set &lt;script type=&quot;math/tex&quot;&gt;\theta_i = H&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;For the Ising model, we can see that there are two sets of model parameters. The spin-spin interaction parameter multiplied by the inverse temperature &lt;script type=&quot;math/tex&quot;&gt;\beta J&lt;/script&gt; controls the effects of each edge in the graph. The inverse temperature multiplied by the magnetic field &lt;script type=&quot;math/tex&quot;&gt;\beta H&lt;/script&gt; affects each spin independently. We can also say that the inverse temperature &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; is a global model parameter. For a fixed interaction and magnetic field, we can vary the temperature to index a specific model.&lt;/p&gt;

&lt;p&gt;This is a subtle but important point. Our joint distribution over the set of random variables (the &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; spins) is &lt;em&gt;indexed&lt;/em&gt; by the set of model parameters. By varying the inverse temperature parameter &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, we are actually selecting a specific model (the Ising model at that temperature). Ditto for a specific choice of the spin-spin interaction parameter &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;what-questions-can-we-ask-about-the-model&quot;&gt;What questions can we ask about the model?&lt;/h3&gt;

&lt;p&gt;Computing the magnetization &lt;script type=&quot;math/tex&quot;&gt;m = \frac{1}{N}\langle  s_1 + ... + s_N \rangle = \langle  s_i \rangle&lt;/script&gt; means calculating the expectation &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_{p(s_i)}[s_i]&lt;/script&gt;. In probability language, this means calculating the marginal expectation of a node &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;But calculating the marginal distribution is intractable for reasons we already discussed: it requires marginalizing over all other nodes &lt;script type=&quot;math/tex&quot;&gt;j \neq i&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s_i) = \sum_{s_1=\pm 1} ... \sum_{s_{i-1}=\pm 1}\sum_{s_{i+1}=\pm 1}...\sum_{s_N=\pm1} p(s_1,...,s_{i-1}, s_i, s_{i+1}, ..., s_N)&lt;/script&gt;

&lt;p&gt;The situation is hopeless: not only do we need to calculate the normalizing constant for the joint distribution of &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; nodes, which has &lt;script type=&quot;math/tex&quot;&gt;2^N&lt;/script&gt; terms, but then we need to marginalize over &lt;script type=&quot;math/tex&quot;&gt;N-1&lt;/script&gt; variables (another &lt;script type=&quot;math/tex&quot;&gt;2^{N-1}&lt;/script&gt; terms).&lt;/p&gt;

&lt;p&gt;This is identical to what we saw in the partition function, when thinking about this model from a physics perspective.&lt;/p&gt;

&lt;p&gt;Can we still answer questions about the marginal distributions by resorting to a variational principle?&lt;/p&gt;

&lt;h3 id=&quot;variational-inference-in-machine-learning&quot;&gt;Variational inference in machine learning&lt;/h3&gt;

&lt;p&gt;If we could calculate the sum over all configurations of random variables, we could calculate the partition function. But we can’t, because the sum grows as &lt;script type=&quot;math/tex&quot;&gt;2^N&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;With our physics hat on, our strategy was to approximate to the partition function.&lt;/p&gt;

&lt;p&gt;From a machine learning perspective, this technique is known as &lt;strong&gt;variational inference&lt;/strong&gt;. We &lt;em&gt;vary&lt;/em&gt; something simple to &lt;em&gt;infer&lt;/em&gt; something complicated.&lt;/p&gt;

&lt;p&gt;Let’s look at how the variational free energy is derived in machine learning and used to approximate partition functions.&lt;/p&gt;

&lt;p&gt;We have a probability model of random variables &lt;script type=&quot;math/tex&quot;&gt;p_\theta(s_1, ..., s_N)&lt;/script&gt; and we seek to calculate its normalizing constant or partition function &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Let’s construct a simpler probability distribution &lt;script type=&quot;math/tex&quot;&gt;q_\lambda(s_1, ..., s_N)&lt;/script&gt;, parameterized by &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, and use it to approximate our model.&lt;/p&gt;

&lt;p&gt;How good is our approximation? One way of measuring how close our approximation is to our goal distribution is with the Kullback-Leibler divergence.&lt;/p&gt;

&lt;p&gt;This divergence between &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, or relative entropy, measures the amount of information (in bits or nats) that is lost when using &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; to approximate &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This gives us a criteria with which to vary our approximation. We the &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; parameter of our approximation until we minimize the approximation error, as measured by the Kullback-Leibler divergence.&lt;/p&gt;

&lt;p&gt;The KL divergence is written with a double vertical bar as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{KL}(q(s) \mid\mid p(s)) = \int q(s) \log \frac{q(s)}{p(s)}ds&lt;/script&gt;

&lt;p&gt;Let’s assume we are dealing with an exponential family distribution such as the Ising model. We let &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; be the Boltzmann distribution for our model with the known energy function &lt;script type=&quot;math/tex&quot;&gt;E(s_1, ..., s_N)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(s) = \frac{e^{-\beta E(s)}}{Z}&lt;/script&gt;

&lt;p&gt;We assume that &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is a family of distributions with another energy function that has parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_\lambda(s) = \frac{e^{-\beta E_\lambda(s)}}{Z_q}&lt;/script&gt;

&lt;p&gt;To measure how much information we lose when we use our approximation &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; instead of &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, we plug them into the Kullback-Leibler divergence:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textrm{KL}(q_\lambda(s) \mid \mid p(s)) = \int q_\lambda(s) \log q_\lambda(s) - \int q_\lambda(s) \log \exp{(-\beta E(s))} + \log Z&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \mathbb{E}_{q_\lambda} [\log q_\lambda(s)] - \mathbb{E}_{q_\lambda}[-\beta E(s)] + \log Z&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= -\mathcal{L(\lambda)} + \log Z&lt;/script&gt;

&lt;p&gt;where we have defined the variational lower bound &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(\lambda)&lt;/script&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\lambda) = \mathbb{E}_{q_\lambda}[\log p(s)] - \mathbb{E}_{q_\lambda}[\log q(s)]&lt;/script&gt;

&lt;p&gt;We can move the variational lower bound to the other side of the equation to get the following identity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log Z = \textrm{KL}(q \mid\mid p) + \mathcal{L}(\lambda)&lt;/script&gt;

&lt;p&gt;With Jensen’s inequality it is easy to show that the KL divergence is always greater than or equal to zero. This means that if we make &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(\lambda)&lt;/script&gt; bigger, the KL divergence must get smaller (i.e. our approximation must improve). Thus we can lower bound the partition function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log Z \geq \mathcal{L}(\lambda)&lt;/script&gt;

&lt;p&gt;This means we can vary the parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; of our approximation to improve the lower bound, and get a better and better approximation to the partition function!&lt;/p&gt;

&lt;p&gt;Note that in the definition of the variational lower bound, we do not need to worry about the arduous task of calculating the partition function: it does not depend on &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;This is awesome: we have constructed an approximation &lt;script type=&quot;math/tex&quot;&gt;q_\lambda&lt;/script&gt; to our probability model &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; and found a way to vary its parameters so that our approximation gets better and better.&lt;/p&gt;

&lt;p&gt;The interesting part is that we get can improve the approximation to our model &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; without calculating its intractable partition function. We &lt;em&gt;only&lt;/em&gt; need to evaluate its energy function &lt;script type=&quot;math/tex&quot;&gt;E(s)&lt;/script&gt; which is cheap to compute.&lt;/p&gt;

&lt;p&gt;Is this too clever to be true? Have we surrendered anything? We have lost the ability to measure how good our approximation is, in absolute terms—for that, we still need to calculate the partition function to compute the KL divergence. We do know that as long as our lower bound &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(\lambda)&lt;/script&gt; increases as we vary &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, our approximation gets better, and this is sufficient for a variety of problems.&lt;/p&gt;

&lt;h3 id=&quot;variational-inference-as-the-gibbs-bogoliubov-feynman-inequality&quot;&gt;Variational inference as the Gibbs-Bogoliubov-Feynman inequality!&lt;/h3&gt;

&lt;p&gt;Let’s see if this is the same as the Gibbs-Bogoliubov-Feynman inequality we saw in physics. Recall that the inequality is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z \geq Z_{MF} \exp{[-\beta \langle E - E_{MF}\rangle_{MF}]}.&lt;/script&gt;

&lt;p&gt;Taking logarithms:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log Z \geq  - \langle \beta E\rangle_{MF} + \langle \beta E_{MF}\rangle_{MF} + \log Z_{MF}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow \log Z \geq \mathbb{E}_{q_\lambda} [ \log p(s) ] - \mathbb{E}_{q_\lambda} [\log q_\lambda(s) ]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow \log Z \geq \mathcal{L}(\lambda)&lt;/script&gt;

&lt;p&gt;Where we have identified that the variational family we are using, is the mean-field Boltzmann distribution &lt;script type=&quot;math/tex&quot;&gt;q_\lambda(s) = \prod_i \frac{\exp(-\beta E_{MF}(s))}{Z_{MF}}&lt;/script&gt;. Again, &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; denotes the variational parameters that we vary to maximize the lower bound &lt;sup id=&quot;fnref:8&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This shows that variational inference in machine learning—maximizing a lower bound on the partition function—is exactly the Gibbs-Bogoliubov-Feynman inequality in action.&lt;/p&gt;

&lt;h3 id=&quot;the-evidence-lower-bound-in-approximate-posterior-inference&quot;&gt;The evidence lower bound in approximate posterior inference&lt;/h3&gt;

&lt;p&gt;In machine learning we care about patterns in data. This gives rise to the concept of &lt;strong&gt;latent variables&lt;/strong&gt;, unobserved random variables that capture patterns in observed data.&lt;/p&gt;

&lt;p&gt;For example, in linear regression we might posit a linear relationship between someone’s age and their income. This scalar coefficient captures a latent pattern that we seek to infer from many examples of (age, income) tuples.&lt;/p&gt;

&lt;p&gt;We refer to a probability model as a model of latent variables &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; and data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. The &lt;strong&gt;posterior distribution&lt;/strong&gt; of latent variables given observed data is written &lt;script type=&quot;math/tex&quot;&gt;p(z \mid x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;What is a posterior? In our regression example of the relationship between age and income, we want the posterior distribution of the regression coefficient after observing data. Our choice of prior on the coefficient is a modeling decision and reflects our belief about the statistical relationship we hope to observe.&lt;/p&gt;

&lt;p&gt;The posterior is given by Bayes’ rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z \mid x) = \frac{p(x \mid z) p(z)}{\int p(x, z) dz}&lt;/script&gt;

&lt;p&gt;The denominator is the evidence; the marginal distribution of the data: &lt;script type=&quot;math/tex&quot;&gt;p(x) = \int p(x, z) dz&lt;/script&gt;. This is the normalizer of the joint distribution of latent variables and data, or the partition function. This partition function is a sum over all configurations of random variables, and is intractable as we saw twice before.&lt;/p&gt;

&lt;p&gt;Can we still do posterior inference despite the intractable partition function?&lt;/p&gt;

&lt;p&gt;The refrain is familiar: we have an intractable sum in our partition function, but we can approximate it using the tools we developed earlier! Variational inference to the rescue. Let’s write out the variational lower bound on the partition function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log Z = \log p(x) \geq \mathcal{L}(\lambda) = \mathbb{E}_{q_\lambda}[p(x, z)] - \mathbb{E}_{q_\lambda}[\log q_\lambda (z)]&lt;/script&gt;

&lt;p&gt;Again, by varying the parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; we can learn a good approximate posterior distribution &lt;script type=&quot;math/tex&quot;&gt;q_\lambda(z)&lt;/script&gt; to approximate the posterior we care about but can’t calculate, &lt;script type=&quot;math/tex&quot;&gt;p(z \mid x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;If we are using the variational method to learn an approximate posterior, our partition function is the evidence &lt;script type=&quot;math/tex&quot;&gt;\log p(x)&lt;/script&gt;. We thus refer to the variational lower bound &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}(\lambda)&lt;/script&gt; as the &lt;strong&gt;Evidence Lower Bound&lt;/strong&gt; or ELBO and speak of maximizing the ELBO to learn a good approximate posterior distribution.&lt;/p&gt;

&lt;p&gt;This technique has been used in machine learning for the past two decades. It is becoming popular because intractable partition functions come with the need to analyze large datasets. Because the variational principle relies on optimizing a lower bound, the field has borrowed heavily from the optimization literature to scale Bayesian inference to massive data. It’s an exciting area, as new techniques from stochastic optimization may enable us to explore new physics and machine learning models.&lt;/p&gt;

&lt;h3 id=&quot;connections-are-machine-learning-techniques-useful-in-physics&quot;&gt;Connections: are machine learning techniques useful in physics?&lt;/h3&gt;

&lt;p&gt;There are many techniques for approximating partition functions developed in the machine learning community that may find use in physics.&lt;/p&gt;

&lt;p&gt;For example, &lt;a href=&quot;https://arxiv.org/abs/1401.0118&quot;&gt;black box variational inference&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1603.00788&quot;&gt;automatic differentiation variational inference&lt;/a&gt; are generic methods that may be useful in physics. They develop frameworks for constructing expressive approximate distributions and efficient optimization techniques.&lt;/p&gt;

&lt;p&gt;Question for physicists familiar with variational methods: is stochastic optimization used in variational methods? &lt;em&gt;Would&lt;/em&gt; this be useful?&lt;/p&gt;

&lt;h3 id=&quot;connections-could-tools-from-physics-be-useful-in-machine-learning&quot;&gt;Connections: could tools from physics be useful in machine learning?&lt;/h3&gt;

&lt;p&gt;Yes! The Gibbs-Bogoliubov-Feynman inequality was originally developed in physics and found its way to machine learning through Michael Jordan’s group at Berkeley.&lt;/p&gt;

&lt;p&gt;There seems to be a separate literature on constructing flexible families of distributions to approximate distributions. The replica trick, renormalization group theory, and others are just some topics that are beginning to make their way from statistical physics to machine learning.&lt;/p&gt;

&lt;p&gt;Another example of tools from physics used in machine learning is &lt;a href=&quot;https://arxiv.org/abs/1610.09033&quot;&gt;operator variational inference&lt;/a&gt;. In this work, we developed a framework for constructing operators (such as the KL divergence) that measure how good an approximation is. The framework enables making explicit the tradeoffs between how good our approximation is and how much computation a variational method requires. The Langevin-Stein operator is equivalent to the Hamiltonian operator in physics (&lt;a href=&quot;https://www.dropbox.com/s/jd844d9ck0yqlgr/operator-hamiltonian-momentum-space.pdf?dl=0&quot;&gt;note&lt;/a&gt;) and was originally developed in a Physical Review Letters &lt;a href=&quot;https://arxiv.org/abs/cond-mat/9911396&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A fun question to ponder is “why KL divergence?” and the physics perspective is illuminating. It corresponds to the first-order Taylor expansion of the partition function and comes with assumptions about the non-equilibrium perturbed distribution. Does the second-order Taylor expansion correspond to another divergence and yield more accurate solutions?&lt;/p&gt;

&lt;p&gt;I recently learned about replica theory. The replica trick is a technique for calculating the partition function of a system exactly, using an &lt;a href=&quot;https://en.wikipedia.org/wiki/Replica_trick&quot;&gt;insane formula&lt;/a&gt;. It begs the question: what assumptions do we need to use this for probabilistic graphical models?&lt;/p&gt;

&lt;p&gt;I’m excited to see more work in this area as physicists &lt;a href=&quot;https://www.wired.com/2017/01/move-coders-physicists-will-soon-rule-silicon-valley/&quot;&gt;migrate&lt;/a&gt; to data science and machine learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;How can we make transitions faster? How can we efficiently move techniques between machine learning and physics? Would code samples be helpful?&lt;/p&gt;

&lt;p&gt;This post is an attempt at mapping the language from one community to another. Another idea is a long review paper that to give detailed examples of models solved within a statistical physics framework (with mean-field methods, replica theory, renormalization theory, etc) and solved with modern variational inference from a machine learning perspective (black box variational inference, stochastic optimization, etc). This would highlight how the fields complement each other.&lt;/p&gt;

&lt;h3 id=&quot;glossary&quot;&gt;Glossary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Expectations: the angle brackets &lt;script type=&quot;math/tex&quot;&gt;\langle ~~\cdot~~\rangle&lt;/script&gt; denote an expectation. In the machine learning literature, this is denoted as &lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}_p[~~\cdot~~]&lt;/script&gt; for the expectation of a quantity with respect to the distribution &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. For example, &lt;script type=&quot;math/tex&quot;&gt;\langle  f(\vec{s}) \rangle&lt;/script&gt; denotes an expectation of a function of the spins &lt;script type=&quot;math/tex&quot;&gt;f(\vec{s})&lt;/script&gt;. The expectation is implicitly with respect to the Boltzmann distribution:
&lt;script type=&quot;math/tex&quot;&gt;\langle  f(\vec{s}) \rangle = \mathbb{E}_p[f(\vec{s})] = \sum_{\{s_1, ..., s_N\}} f(\vec{s}) p(\vec{s})&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \sum_{\{s_1, ..., s_N\}} f(\vec{s})=\frac{e^{-\beta H(\vec{s})}}{Z}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Spins in physics are called random variables in statistics and machine learning.&lt;/li&gt;
  &lt;li&gt;The evidence lower bound in variational inference is the negative free energy in physics terminology.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Anything to add or fix in this article to reduce confusion and increase clarity? Please &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email me&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/thejaan&quot;&gt;tweet&lt;/a&gt;, or &lt;a href=&quot;https://github.com/altosaar/jaan.io/blob/master/_posts/blog/2017-08-11-how-does-physics-connect-machine-learning.md&quot;&gt;submit a pull request&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.complex-systems.com/pdf/01-5-6.pdf&quot;&gt;Peterson &amp;amp; Anderson (1987)&lt;/a&gt; used solutions to time-dependent Ising models to learn the parameters of Boltzmann machines. This is a canonical reference for the ‘start’ of variational inference as it is known in the machine learning community.&lt;/li&gt;
  &lt;li&gt;You can go deep into Ising models: there are hundreds of lectures and references on line. Here are the sources I used for these notes: from &lt;a href=&quot;http://quantumtheory.physik.unibas.ch/people/bruder/Semesterprojekte2007/p1/Ising.pdf&quot;&gt;Basel&lt;/a&gt; and &lt;a href=&quot;https://www.physik.uni-muenchen.de/lehre/vorlesungen/sose_14/asp/lectures/ASP_Chapter5.pdf&quot;&gt;Munich&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Dave’s course, &lt;a href=&quot;http://www.cs.columbia.edu/~blei/fogm/2016F/&quot;&gt;Foundations of Graphical Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf&quot;&gt;Wainwright &amp;amp; Jordan (2008)&lt;/a&gt; is challenging but worthwhile.&lt;/li&gt;
  &lt;li&gt;David Chandler’s Introduction to Modern Statistical Mechanics (1987) has a simple derivation of the variational free energy (Section 5.1, pp. 135-138) that I followed in this exposition.&lt;/li&gt;
  &lt;li&gt;Feynman, Statistical Mechanics - A set of lecture notes (1972) derives the variational free energy using a perturbation expansion (Section 2.11, pp. 67-71).&lt;/li&gt;
  &lt;li&gt;Parisi’s Statistical Field Theory (1988) derives the variational principle in three different ways (Section 3.2, pp. 24-31).&lt;/li&gt;
  &lt;li&gt;Matthew Beal’s &lt;a href=&quot;http://www.cse.buffalo.edu/faculty/mbeal/thesis/beal03_2.pdf&quot;&gt;thesis&lt;/a&gt; has interesting references, and Rich Turner has &lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~turner/Notes/ContrastiveDivergence/FreeEnergyNotes.pdf&quot;&gt;notes&lt;/a&gt; on correspondences between physics and machine learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thanks to Bohdan Kulchytskyy, Florian Wentzel, Smiti Kaul, Guillaume Verdon, Henri Palacci, Sam Ritter, Mattias Fitzpatrick, and Sophie Kleber for comments and encouragement. Image credits: Freepik for iconography, and Analytical Scientific for the Newton’s cradle image.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.physics.udel.edu/~glyde/PHYS813/Lectures/chapter_3.pdf&quot;&gt;Derivation&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;For a tiny system, e.g. with three spins, we have &lt;script type=&quot;math/tex&quot;&gt;8&lt;/script&gt; states and the sum is doable - but the system is uninteresting.&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;For example, the magnetization of dysprosium aluminium garnet at low temperatures is exactly &lt;a href=&quot;http://www.sbfisica.org.br/bjp/files/v30_794.pdf&quot;&gt;described&lt;/a&gt; by this model.&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;To see this, recall that &lt;script type=&quot;math/tex&quot;&gt;\cosh x = \frac{e^x + e^{-x}}{2}&lt;/script&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.wolframalpha.com/input/?i=plot+e%5Ex,+x+%2B+1&quot;&gt;Visual proof&lt;/a&gt; that &lt;script type=&quot;math/tex&quot;&gt;e^x \geq x + 1&lt;/script&gt;.&amp;nbsp;&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;The semicolon notation means “the distribution over &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is parameterized in terms of the parameter &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;”.&amp;nbsp;&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;Writing the parameters of a distribution as a subscript (&lt;script type=&quot;math/tex&quot;&gt;p_\theta(s)&lt;/script&gt;) is shorthand for writing them after the semicolon (&lt;script type=&quot;math/tex&quot;&gt;p(s; \theta)&lt;/script&gt;).&amp;nbsp;&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot;&gt;
      &lt;p&gt;In the variational treatment of the Ising model we had one variational parameter, the perturbation to the static magnetic field &lt;script type=&quot;math/tex&quot;&gt;\lambda = \Delta H&lt;/script&gt;.&amp;nbsp;&lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/how-does-physics-connect-machine-learning/&quot;&gt;How does physics connect to machine learning?&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on August 11, 2017.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[food2vec - Augmented cooking with machine intelligence]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/food2vec-augmented-cooking-machine-intelligence/" />
  <id>http://localhost:4000/food2vec-augmented-cooking-machine-intelligence</id>
  <published>2017-01-22T00:00:00+01:00</published>
  <updated>2017-01-22T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;TL;DR: Check out the &lt;a href=&quot;https://altosaar.github.io/food2vec/&quot;&gt;tools demo&lt;/a&gt; to explore food analogies and recommendations, or scroll down for an interactive map of a hundred thousand recipes from around the world.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I haven’t eaten in five days. I dream of food. I study food. Deep in ketosis, my body has adapted to consume itself: I am food. There is no better time to dig into modeling grub.&lt;/p&gt;

&lt;p&gt;Machine intelligence has changed your life, from how you listen to music through Discover Weekly playlists, consume news through Facebook, or talk to your hand computer’s friendly digital assistant. But why hasn’t it changed how we eat? Can we modify the ingredients of language processing algorithms to get insights about food? If you tell me what you want to eat, can I recommend complementary foods, much like Spotify recommends complementary songs?&lt;/p&gt;

&lt;p&gt;Word embeddings are a useful technique for analyzing discrete data. Say we use &lt;script type=&quot;math/tex&quot;&gt;170,000&lt;/script&gt; words from the Oxford English dictionary. We can represent each word (such as “food”) as a vector as follows: a list of &lt;script type=&quot;math/tex&quot;&gt;169,999&lt;/script&gt; zeros, with a single &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; at the location of the word in the vocabulary. In our case, “food” may be at location &lt;script type=&quot;math/tex&quot;&gt;29,163&lt;/script&gt; near other words beginning with the letter f. Then the vector for “food” would look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;[0, 0, 0, ..., 0, 0, 1, 0, 0, ..., 0].&lt;/script&gt;

&lt;p&gt;However, this is inadequate for comparing words. To compare documents and get useful insights from our data, we need to aggregate over &lt;script type=&quot;math/tex&quot;&gt;170,000&lt;/script&gt; dimensions for each word, which takes far too long. Can we do better?&lt;/p&gt;

&lt;p&gt;Embeddings let us reduce the dimensionality of the problem, and give us a powerful representation of language. We can build a model of language where we assign a hundred random numbers to each word. To train the model, we use these hundred numbers of each word to predict their context. The “context” of a word consists of its surrounding words. This is the main idea: the context means that words that occur in similar contexts should have similar meanings. We tweak the numbers assigned to a word to make them better at predicting words in the context. Initially, the random numbers assigned to a word will be bad at predicting words in the context. But gradually, through this process of tweaking the model’s predictions of surrounding words, we get a hundred numbers that are far from random. The hundred numbers representing each word will capture part of its meaning: similar words will cluster together because they occur in each other’s contexts, and words with different meanings are pushed far apart (out-of-context). By representing each word as an embedding in &lt;script type=&quot;math/tex&quot;&gt;100&lt;/script&gt; dimensions, we have reduced the dimensionality more than a thousandfold from &lt;script type=&quot;math/tex&quot;&gt;170,000&lt;/script&gt; and gained a better representation of language.&lt;/p&gt;

&lt;p&gt;For modeling food, we have a collection of recipes. We can define the context of an ingredient in a recipe to be the rest of the foods in the recipe. This demonstrates the flexibility of embeddings: by making a small change in the definition of the context, we can now apply it to a totally different kind of data.&lt;/p&gt;

&lt;h3 id=&quot;food-similarity-map&quot;&gt;Food similarity map&lt;/h3&gt;

&lt;p&gt;After training the embedding algorithm on a collection of &lt;script type=&quot;math/tex&quot;&gt;95, 896&lt;/script&gt; recipes, we get &lt;script type=&quot;math/tex&quot;&gt;100&lt;/script&gt;-dimensional embeddings for each food. Humans can’t visualize high dimensions, so we use an approximation technique to visualize similarity between the foods in two dimensions.&lt;/p&gt;

&lt;p&gt;Here is a similarity map of the &lt;script type=&quot;math/tex&quot;&gt;2,087&lt;/script&gt; ingredients used in the recipes. Hover over a point to see which food it represents:&lt;/p&gt;

&lt;iframe width=&quot;100%&quot; height=&quot;800&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;/files/food2vec_food_embeddings_tsne.html&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The map of foods is reasonable. Ingredients from Asia cluster together, as do ingredients used in European and North American cooking.&lt;/p&gt;

&lt;h3 id=&quot;recipe-embedding-map&quot;&gt;Recipe embedding map&lt;/h3&gt;

&lt;p&gt;We can generate an embedding for a recipe by taking the average of its ingredients’ embeddings. Here is a map of &lt;script type=&quot;math/tex&quot;&gt;95, 896&lt;/script&gt; recipes from around the world. Hover over a point to see the recipe, and click on the cuisine legend on the right to show or hide certain regions:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;IMPORTANT: you are about to download 15MB of data.&lt;/em&gt; Click &lt;a href=&quot;/files/food2vec_recipe_embeddings_tsne.html&quot;&gt;here&lt;/a&gt; to access the map, zoom in, and discover new flavors. Is this the fastest way to browse 100k recipes by similarity?&lt;/p&gt;

&lt;p&gt;Interesting patterns emerge. Asian recipes cluster together, as do Southern European recipes. Northern European and American foods are all over the place, maybe because of transmission of recipes due to migration, or over-representation in the data.&lt;/p&gt;

&lt;h3 id=&quot;food-similarity-tool&quot;&gt;Food similarity tool&lt;/h3&gt;

&lt;p&gt;Access the tool at &lt;a href=&quot;https://altosaar.github.io/food2vec/#food-similarity-tool&quot;&gt;this link&lt;/a&gt;. We can calculate food similarity by looking at which food is closest in the high dimensional space in the embeddings.&lt;/p&gt;

&lt;p&gt;These mostly make sense - foods are closest to other foods they appear with in recipes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Cheese is closest to macaroni&lt;/li&gt;
  &lt;li&gt;Sesame oil is closest to egg noodle&lt;/li&gt;
  &lt;li&gt;Milk is closest to nutmeg&lt;/li&gt;
  &lt;li&gt;Olive oil is closest to parmesan cheese&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;food-analogy-tool&quot;&gt;Food analogy tool&lt;/h3&gt;

&lt;p&gt;Access the tool &lt;a href=&quot;https://altosaar.github.io/food2vec/#food-analogy-tool&quot;&gt;here&lt;/a&gt;. Food analogies, like word analogies, are calculated with vector arithmetic. For the analogy “Food A is to food B, as food C is to food D”, the goal is to predict a reasonable food D. We can do this by subtracting food B from food A, then adding food C. For example, calculating &lt;script type=&quot;math/tex&quot;&gt;(bacon - egg) + orangejuice&lt;/script&gt; in embedding space will yield an embedding. The closest embedding to this is &lt;script type=&quot;math/tex&quot;&gt;coffee&lt;/script&gt; in our model of food. The classic example from word embeddings is &lt;script type=&quot;math/tex&quot;&gt;(king - man) + woman = queen&lt;/script&gt;. Is this intuitive? King is to man as woman is to queen makes sense in natural language, but food analogies are less clear. With practice, we may be able to train our taste detectors and devise hypotheses to test in the realm of food. I also included cuisine embeddings by representing them as the average of their recipes’ embeddings.&lt;/p&gt;

&lt;p&gt;Some of these are more plausible than others:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Egg is to bacon as orange juice is to coffee.&lt;/li&gt;
  &lt;li&gt;Bread is to butter as roast beef is to sage.&lt;/li&gt;
  &lt;li&gt;Smoked salmon is to dill as lamb is to asparagus.&lt;/li&gt;
  &lt;li&gt;South Asian is to rice as Southern European is to thyme.&lt;/li&gt;
  &lt;li&gt;Rice is to sesame seed as macaroni is to pimento.&lt;/li&gt;
  &lt;li&gt;Roasted beef is to green bell pepper as pork sausage is to fenugreek.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;recipe-recommendation-tool&quot;&gt;Recipe recommendation tool&lt;/h3&gt;

&lt;p&gt;Access the tool &lt;a href=&quot;https://altosaar.github.io/food2vec/#recipe-recommendation-tool&quot;&gt;here&lt;/a&gt;. We can use our model of food as a recommendation system for cooks. By taking the average embedding for a set of foods, we can look up foods with the closest embeddings.&lt;/p&gt;

&lt;p&gt;For example, I am a lifelong aficionado of peanut butter jam sandwiches. I entered my usual favorite: white bread, butter, peanut butter, honey. The top recommendation was: strawberry. I’ve never tried that, and it’s pretty good! I happily broke my fast with it. For the recipe of lamb, cumin, tomato, the top recommendation is raisin - also reasonable and interesting. Other recommendations are a bit wackier, so best of luck.&lt;/p&gt;

&lt;p&gt;If you end up adding an ingredient to your food based on these tools, I’d love to hear how it tasted: ping me on &lt;a href=&quot;https://twitter.com/thejaan&quot;&gt;Twitter&lt;/a&gt; or &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email&lt;/a&gt;!&lt;/p&gt;

&lt;h3 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Figuring out the right user interface to explore these models. The code for the plots and recommendation tools is on &lt;a href=&quot;https://github.com/altosaar/food2vec&quot;&gt;github&lt;/a&gt;. It would be great to make these mobile-friendly and test other ways of presenting recommendations from the model to users.&lt;/li&gt;
  &lt;li&gt;word2vec is not the best model for this. Multi-class regression should work well, and I added a working &lt;a href=&quot;https://github.com/altosaar/food2vec/blob/master/src/food2vec.py&quot;&gt;demo of this&lt;/a&gt; to the repo. This is a rare case where the vocabulary size (number of ingredients) is very small, so we can fit both models and compare them. This could reveal idiosyncrasies in the non-contrastive estimation loss used in word2vec and provides an interesting testbed.&lt;/li&gt;
  &lt;li&gt;Scaling up the data:  Do you have a larger dataset of recipes, or do you know how to scrape one? I’d love to check it out. This would also fix bias in the data as the majority of the recipes are currently North American.&lt;/li&gt;
  &lt;li&gt;Testing out recipe analogies combined with food analogies: this may be more intuitive for us humans. For example, “pancakes are to maple syrup, as an omelette is to cheese” could be easier to think about than analogies with individual ingredients.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;This NYT piece, &lt;a href=&quot;https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html&quot;&gt;The Great AI Awakening&lt;/a&gt;, does a much better job at describing embeddings than I can&lt;/li&gt;
  &lt;li&gt;Wesley has a neat paper on a similar approach: &lt;a href=&quot;https://arxiv.org/abs/1612.00388&quot;&gt;diet2vec&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sanjeev Arora’s &lt;a href=&quot;http://www.offconvex.org/2016/02/14/word-embeddings-2/&quot;&gt;research&lt;/a&gt; has good explanations for the analogy properties of embeddings&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm&quot;&gt;t-SNE algorithm&lt;/a&gt; for visualizing high-dimensional embeddings&lt;/li&gt;
  &lt;li&gt;The original &lt;a href=&quot;http://www.nature.com/articles/srep00196&quot;&gt;Nature Scientific Report&lt;/a&gt; with the data&lt;/li&gt;
  &lt;li&gt;Dave taught a fantastic &lt;a href=&quot;http://www.cs.columbia.edu/~blei/seminar/2016_discrete_data/index.html&quot;&gt;class&lt;/a&gt; that helped me understand embeddings&lt;/li&gt;
  &lt;li&gt;Maja’s paper on &lt;a href=&quot;https://arxiv.org/abs/1608.00778&quot;&gt;exponential family embeddings&lt;/a&gt; generalizes word2vec to other distributions that would be neat to try on this data (word2vec can be interpreted as a Bernoulli embedding model with biased gradients)&lt;/li&gt;
  &lt;li&gt;There are a few other versions of food2vec floating around, like &lt;a href=&quot;https://automateddeveloper.blogspot.com/2016/10/unsupervised-learning-in-scala-using.html&quot;&gt;Rob Hinds’&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&quot;http://www.cs.columbia.edu/~blei/&quot;&gt;Dave&lt;/a&gt; for the idea, &lt;a href=&quot;http://sociology.columbia.edu/node/66&quot;&gt;Peter Bearman&lt;/a&gt; for presenting his work to our group, &lt;a href=&quot;https://www.flickr.com/photos/mealmakeovermoms/&quot;&gt;MealMakeOverMoms&lt;/a&gt; for the mise photo, &lt;a href=&quot;http://anthony.ai/&quot;&gt;Anthony&lt;/a&gt; for open-sourcing the embedding browser on which ours is based,  and &lt;a href=&quot;https://plot.ly/&quot;&gt;Plotly&lt;/a&gt; for open-sourcing their fantastic plotting library.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Feel free to ping me on &lt;a href=&quot;https://twitter.com/thejaan&quot;&gt;Twitter&lt;/a&gt; or &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email&lt;/a&gt; with feedback or ideas!&lt;/p&gt;

&lt;p&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=13472721&quot;&gt;Hacker News&lt;/a&gt; and &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/5px9uz/p_food_visualization_and_recommendation_engine_in/&quot;&gt;Reddit&lt;/a&gt;. Also see &lt;a href=&quot;https://github.com/altosaar/food2vec/blob/master/doc/food2vec-nytimes-talk.pdf&quot;&gt;slides&lt;/a&gt; from a talk at the New York Times on this project.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/food2vec-augmented-cooking-machine-intelligence/&quot;&gt;food2vec - Augmented cooking with machine intelligence&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on January 22, 2017.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Operator Variational Inference]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/operator-variational-inference/" />
  <id>http://localhost:4000/operator-variational-inference</id>
  <published>2016-10-31T00:00:00+01:00</published>
  <updated>2016-10-31T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;New divergences for variational inference that make explicit tradeoffs such as computation and performance.&lt;/p&gt;

&lt;p&gt;R. Ranganath, J. Altosaar, D. Tran, D. Blei. Operator Variational Inference. NIPS, 2016.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/2016_Ranganath-Altosaar-Tran-Blei_OperatorVI.pdf&quot;&gt;&lt;i class=&quot;fa fa-file-pdf-o&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt; &lt;a href=&quot;https://github.com/blei-lab/publications/tree/master/2016_RanganathAltosaarTranBlei&quot;&gt;&lt;i class=&quot;fa fa-wrench&quot;&gt;&lt;/i&gt; LaTeX Source&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1610.09033&quot;&gt;&lt;i class=&quot;fa fa-institution&quot;&gt;&lt;/i&gt; arXiv&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/operator-variational-inference/&quot;&gt;Operator Variational Inference&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on October 31, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Word embedding models for recommendation]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/factorization-item-embedding/" />
  <id>http://localhost:4000/factorization-item-embedding</id>
  <published>2016-08-05T00:00:00+02:00</published>
  <updated>2016-08-05T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;Ideas from word embeddings can boost recommendation performance in matrix factorization systems.&lt;/p&gt;

&lt;p&gt;Dawen Liang, Jaan Altosaar, Laurent Charlin, and David Blei. Factorization meets the item embedding. &lt;a href=&quot;https://recsys.acm.org/recsys16/&quot;&gt;Recsys 2016&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/2016_Liang-Altosaar-Charlin-Blei_CoFactor.pdf&quot;&gt;&lt;i class=&quot;fa fa-file-pdf-o&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt; &lt;a href=&quot;https://github.com/dawenl/cofactor&quot;&gt;&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt; Github&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/factorization-item-embedding/&quot;&gt;Word embedding models for recommendation&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on August 05, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Variational Autoencoder Perspectives]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/variational-autoencoder-perspectives/" />
  <id>http://localhost:4000/variational-autoencoder-perspectives</id>
  <published>2016-07-24T00:00:00+02:00</published>
  <updated>2016-07-24T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">### Takeaway: why the neural net perspective limits us

I hope you are convinced that reasoning about the variational autoencoder is less ambiguous and less confusing from the perspective of variational inference in probability models. In neural net language, the variational autoencoder refers to an encoder, a decoder, and a loss function. In probability model terms, the variational autoencoder refers to approximate inference in a latent Gaussian model, where the approximate posterior and model likelihood are parametrized by neural nets (the inference and generative networks). The sentence describing the variational autoencoder in neural net terms is unclear: What is the encoder? What does the decoder mean? What is the loss function? Each term requires further explanation. In contrast, the probability model language gives us an objective function (the ELBO) for free, and we can simply state that we parametrize the approximate posterior and model with neural nets.

Here are more reasons why we should favor the probability model perspective on variational autoencoders:

* *Separating model and inference*: Shakir [makes this point well](http://blog.shakirm.com/2015/03/a-statistical-view-of-deep-learning-ii-auto-encoders-and-free-energy/). Rather than being limited to an 'encoder' in neural net terms, we can think of the probability model at hand, $$ p(x, z) $$ separately from the approximate inference scheme. This lets us choose from a variety of methods, rather than thinking only in terms of amortized inference using a neural net. It is our choice whether to explore other (perhaps better) methods such as mean-field variational inference or MCMC/HMC/Langevin dynamics to learn the parameters of the model.
* *Composability*: the moment we add a second layer of latent variables to our model that depend on the first layer, the encoder/decoder framework breaks down. How should we parametrize the inference network? Can we still do amortized inference? The framework of probability models can help us use build more complex models from basic building blocks, and gives us clear frameworks for how to do inference. Thinking in terms of encoders is dangerous for top-down inference, as it is unclear how to parametrize the encoder for any more than one layer of latent variables.
* *Regularization is free*: in neural net terms, we discussed 'regularizer' term in the loss function (the KL divergence between the approximate posterior and prior). This comes out of the blue if one is not familiar with variational inference. But in probability model language, it is simply and alternate form of the ELBO, and we can immediately think about alternative priors that may be more appropriate for the data we wish to model.
  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/variational-autoencoder-perspectives/&quot;&gt;Variational Autoencoder Perspectives&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on July 24, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Tutorial - What is a variational autoencoder?]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/what-is-variational-autoencoder-vae-tutorial/" />
  <id>http://localhost:4000/what-is-variational-autoencoder-vae-tutorial</id>
  <published>2016-07-18T00:00:00+02:00</published>
  <updated>2016-07-18T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;Why do deep learning researchers and probabilistic machine learning folks get confused when discussing variational autoencoders? What is a variational autoencoder? Why is there unreasonable confusion surrounding this term?&lt;/p&gt;

&lt;p&gt;There is a conceptual and language gap. The sciences of neural networks and probability models do not have a shared language. My goal is to bridge this idea gap and allow for more collaboration and discussion between these fields, and provide a consistent implementation (&lt;a href=&quot;https://github.com/altosaar/vae/blob/master/vae.py&quot;&gt;Github link&lt;/a&gt;). &lt;em&gt;If many words here are new to you, jump to the &lt;a href=&quot;#glossary&quot;&gt;glossary&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Variational autoencoders are cool. They let us design complex generative models of data, and fit them to large datasets. They can generate images of fictional celebrity faces and high-resolution &lt;a href=&quot;http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/&quot;&gt;digital artwork&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/variational-autoencoder-faces.jpg&quot; style=&quot;max-width: 50%&quot; /&gt;
    &lt;figcaption&gt;Fictional celebrity faces generated by a variational autoencoder (&lt;a href=&quot;https://www.youtube.com/watch?v=XNZIN7Jh3Sg&quot;&gt;by Alec Radford&lt;/a&gt;). &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;These models also yield state-of-the-art machine learning results in &lt;a href=&quot;https://arxiv.org/abs/1502.04623&quot;&gt;image generation&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1509.08731&quot;&gt;reinforcement learning&lt;/a&gt;. Variational autoencoders (VAEs) were defined in 2013 by &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Kingma et al.&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1401.4082&quot;&gt;Rezende et al.&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;How can we create a language for discussing variational autoencoders? Let’s think about them first using neural networks, then using variational inference in probability models.&lt;/p&gt;

&lt;h3 id=&quot;the-neural-net-perspective&quot;&gt;The neural net perspective&lt;/h3&gt;

&lt;p&gt;In neural net language, a variational autoencoder consists of an encoder, a decoder, and a loss function.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/encoder-decoder.png&quot; /&gt;
    &lt;figcaption&gt;The encoder compresses data into a latent space (z). The decoder reconstructs the data given the hidden representation.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The &lt;em&gt;encoder&lt;/em&gt; is a neural network. Its input is a datapoint &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, its output is a hidden representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, and it has weights and biases &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;. To be concrete, let’s say &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is a 28 by 28-pixel photo of a handwritten number. The encoder ‘encodes’ the data which is &lt;script type=&quot;math/tex&quot;&gt;784&lt;/script&gt;-dimensional into a latent (hidden) representation space &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, which is much less than &lt;script type=&quot;math/tex&quot;&gt;784&lt;/script&gt; dimensions. This is typically referred to as a ‘bottleneck’ because the encoder must learn an efficient compression of the data into this lower-dimensional space. Let’s denote the encoder &lt;script type=&quot;math/tex&quot;&gt;q_\theta (z \vert x)&lt;/script&gt;. We note that the lower-dimensional space is stochastic: the encoder outputs parameters to &lt;script type=&quot;math/tex&quot;&gt;q_\theta (z \vert x)&lt;/script&gt;, which is a Gaussian probability density. We can sample from this distribution to get noisy values of the representations &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;decoder&lt;/em&gt; is another neural net. Its input is the representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, it outputs the parameters to the probability distribution of the data, and has weights and biases &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. The decoder is denoted by &lt;script type=&quot;math/tex&quot;&gt;p_\phi(x\vert z)&lt;/script&gt;. Running with the handwritten digit example, let’s say the photos are black and white and represent each pixel as &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. The probability distribution of a single pixel can be then represented using a Bernoulli distribution. The decoder gets as input the latent representation of a digit &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; and outputs &lt;script type=&quot;math/tex&quot;&gt;784&lt;/script&gt; Bernoulli parameters, one for each of the &lt;script type=&quot;math/tex&quot;&gt;784&lt;/script&gt; pixels in the image. The decoder ‘decodes’ the real-valued numbers in &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; into &lt;script type=&quot;math/tex&quot;&gt;784&lt;/script&gt; real-valued numbers between &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;. Information is lost because it goes from a smaller to a larger dimensionality. How much information is lost? We measure this using the reconstruction log-likelihood &lt;script type=&quot;math/tex&quot;&gt;\log p_\phi (x\vert z)&lt;/script&gt; whose units are nats. This measure tells us how effectively the decoder has learned to reconstruct an input image &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; given its latent representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;loss function&lt;/em&gt; of the variational autoencoder is the negative log-likelihood with a regularizer. Because there are no global representations that are shared by all datapoints, we can decompose the loss function into only terms that depend on a single datapoint &lt;script type=&quot;math/tex&quot;&gt;l_i&lt;/script&gt;. The total loss is then &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N l_i&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; total datapoints. The loss function &lt;script type=&quot;math/tex&quot;&gt;l_i&lt;/script&gt; for datapoint &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_i(\theta, \phi) = - E_{z\sim q_\theta(z\vert x_i)}[\log p_\phi(x_i\vert z)] + KL(q_\theta(z\vert x_i) \vert\vert p(z))&lt;/script&gt;

&lt;p&gt;The first term is the reconstruction loss, or expected negative log-likelihood of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;-th datapoint. The expectation is taken with respect to the encoder’s distribution over the representations. This term encourages the decoder to learn to reconstruct the data. If the decoder’s output does not reconstruct the data well, it will incur a large cost in this loss function.&lt;/p&gt;

&lt;p&gt;The second term is a regularizer that we throw in (we’ll see how it’s derived later). This is the Kullback-Leibler divergence between the encoder’s distribution &lt;script type=&quot;math/tex&quot;&gt;q_\theta(z\vert x)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(z)&lt;/script&gt;. This divergence measures how much information is lost (in units of nats) when using &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; to represent &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;. It is one measure of how close &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; is to &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In the variational autoencoder, &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is specified as a standard Normal distribution with mean zero and variance one, or &lt;script type=&quot;math/tex&quot;&gt;p(z) = Normal(0,1)&lt;/script&gt;. If the encoder outputs representations &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; that are different than those from a standard normal distribution, it will receive a penalty in the loss. This regularizer term means ‘keep the representations &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; of each digit sufficiently diverse’. If we didn’t include the regularizer, the encoder could learn to cheat and give each datapoint a representation in a different region of Euclidean space. This is bad, because then two images of the same number (say a 2 written by different people, &lt;script type=&quot;math/tex&quot;&gt;2_{alice}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;2_{bob}&lt;/script&gt;) could end up with very different representations &lt;script type=&quot;math/tex&quot;&gt;z_{alice}, z_{bob}&lt;/script&gt;. We want the representation space of &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; to be meaningful, so we penalize this behavior. This has the effect of keeping similar numbers’ representations close together (e.g. so the representations of the digit two &lt;script type=&quot;math/tex&quot;&gt;{z_{alice}, z_{bob}, z_{ali}}&lt;/script&gt; remain sufficiently close).&lt;/p&gt;

&lt;p&gt;We train the variational autoencoder using gradient descent to optimize the loss with respect to the parameters of the encoder and decoder &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;. For stochastic gradient descent with step size &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt;, the encoder parameters are updated using &lt;script type=&quot;math/tex&quot;&gt;\theta \leftarrow \theta - \rho \frac{\partial l}{\partial \theta}&lt;/script&gt; and the decoder is updated similarly.&lt;/p&gt;

&lt;h3 id=&quot;the-probability-model-perspective&quot;&gt;The probability model perspective&lt;/h3&gt;

&lt;p&gt;Now let’s think about variational autoencoders from a probability model perspective. Please forget everything you know about deep learning and neural networks for now. Thinking about the following concepts in isolation from neural networks will clarify things. At the very end, we’ll bring back neural nets.&lt;/p&gt;

&lt;p&gt;In the probability model framework, a variational autoencoder contains a specific probability model of data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and latent variables &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. We can write the joint probability of the model as &lt;script type=&quot;math/tex&quot;&gt;p(x, z) = p(x \vert z) p(z)&lt;/script&gt;. The generative process can be written as follows.&lt;/p&gt;

&lt;p&gt;For each datapoint &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Draw latent variables &lt;script type=&quot;math/tex&quot;&gt;z_i \sim p(z)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Draw datapoint &lt;script type=&quot;math/tex&quot;&gt;x_i \sim p(x\vert z)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can represent this as a graphical model:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/graphical-model-variational-autoencoder.png&quot; width=&quot;50%&quot; height=&quot;40px&quot; style=&quot;max-width: 40%&quot; /&gt;
    &lt;figcaption&gt;The graphical model representation of the model in the variational autoencoder. The latent variable z is a standard normal, and the data are drawn from p(x|z). The shaded node for X denotes observed data. For black and white images of handwritten digits, this data likelihood is Bernoulli distributed. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This is the central object we think about when discussing variational autoencoders from a probability model perspective. The latent variables are drawn from a prior &lt;script type=&quot;math/tex&quot;&gt;p(z)&lt;/script&gt;. The data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; have a likelihood &lt;script type=&quot;math/tex&quot;&gt;p(x \vert z)&lt;/script&gt; that is conditioned on latent variables &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. The model defines a joint probability distribution over data and latent variables: &lt;script type=&quot;math/tex&quot;&gt;p(x, z)&lt;/script&gt;. We can decompose this into the likelihood and prior: &lt;script type=&quot;math/tex&quot;&gt;p(x,z) = p(x\vert z)p(z)&lt;/script&gt;. For black and white digits, the likelihood is Bernoulli distributed.&lt;/p&gt;

&lt;p&gt;Now we can think about inference in this model. The goal is to infer good values of the latent variables given observed data, or to calculate the posterior &lt;script type=&quot;math/tex&quot;&gt;p(z \vert x)&lt;/script&gt;. Bayes says:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(z \vert x) = \frac{p(x \vert z)p(z)}{p(x)}.&lt;/script&gt;

&lt;p&gt;Examine the denominator &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;. This is called the evidence, and we can calculate it by marginalizing out the latent variables: &lt;script type=&quot;math/tex&quot;&gt;p(x) = \int p(x \vert z) p(z) dz&lt;/script&gt;. Unfortunately, this integral requires exponential time to compute as it needs to be evaluated over all configurations of latent variables. We therefore need to approximate this posterior distribution.&lt;/p&gt;

&lt;p&gt;Variational inference approximates the posterior with a family of distributions &lt;script type=&quot;math/tex&quot;&gt;q_\lambda(z \vert x)&lt;/script&gt;. The variational parameter &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; indexes the family of distributions. For example, if &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; were Gaussian, it would be the mean and variance of the latent variables for each datapoint &lt;script type=&quot;math/tex&quot;&gt;\lambda_{x_i} = (\mu_{x_i}, \sigma^2_{x_i}))&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;How can we know how well our variational posterior &lt;script type=&quot;math/tex&quot;&gt;q(z \vert x)&lt;/script&gt; approximates the true posterior &lt;script type=&quot;math/tex&quot;&gt;p(z \vert x)&lt;/script&gt;? We can use the Kullback-Leibler divergence, which measures the information lost when using &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; to approximate &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; (in units of nats):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(q_\lambda(z \vert x) \vert \vert p(z \vert x)) =&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{E}_q[\log q_\lambda(z \vert x)]- \mathbf{E}_q[\log p(x, z)] + \log p(x)&lt;/script&gt;

&lt;p&gt;Our goal is to find the variational parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; that minimize this divergence. The optimal approximate posterior is thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_\lambda^* (z \vert x) = {\arg\min}_\lambda KL(q_\lambda(z \vert x) \vert \vert p(z \vert x)).&lt;/script&gt;

&lt;p&gt;Why is this impossible to compute directly? The pesky evidence &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt; appears in the divergence. This is intractable as discussed above. We need one more ingredient for tractable variational inference. Consider the following function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ELBO(\lambda) = \mathbf{E}_q[\log p(x, z)] - \mathbf{E}_q[\log q_\lambda(z \vert x)].&lt;/script&gt;

&lt;p&gt;Notice that we can combine this with the Kullback-Leibler divergence and rewrite the evidence as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log p(x) = ELBO(\lambda) + KL(q_\lambda(z \vert x) \vert \vert p(z \vert x))&lt;/script&gt;

&lt;p&gt;By Jensen’s inequality, the Kullback-Leibler divergence is always greater than or equal to zero. This means that minimizing the Kullback-Leibler divergence is equivalent to maximizing the ELBO. The abbreviation is revealed: the Evidence Lower BOund allows us to do approximate posterior inference. We are saved from having to compute and minimize the Kullback-Leibler divergence between the approximate and exact posteriors. Instead, we can maximize the ELBO which is equivalent (but computationally tractable).&lt;/p&gt;

&lt;p&gt;In the variational autoencoder model, there are only local latent variables (no datapoint shares its latent &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; with the latent variable of another datapoint). So we can decompose the ELBO into a sum where each term depends on a single datapoint. This allows us to use stochastic gradient descent with respect to the parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; (important: the variational parameters are shared across datapoints - more on this &lt;a href=&quot;#mean-field&quot;&gt;here&lt;/a&gt;). The ELBO for a single datapoint in the variational autoencoder is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ELBO_i(\lambda) = E_{q_\lambda(z\vert x_i)}[\log p(x_i\vert z)] - KL(q_\lambda(z\vert x_i) \vert\vert p(z)).&lt;/script&gt;

&lt;p&gt;To see that this is equivalent to our previous definition of the ELBO, expand the log joint into the prior and likelihood terms and use the product rule for the logarithm.&lt;/p&gt;

&lt;p&gt;Let’s make the connection to neural net language. The final step is to parametrize the approximate posterior &lt;script type=&quot;math/tex&quot;&gt;q_\theta (z \vert x, \lambda)&lt;/script&gt; with an &lt;em&gt;inference network&lt;/em&gt; (or encoder) that takes as input data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and outputs parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. We parametrize the likelihood &lt;script type=&quot;math/tex&quot;&gt;p(x \vert z)&lt;/script&gt; with a &lt;em&gt;generative network&lt;/em&gt; (or decoder) that takes latent variables and outputs parameters to the data distribution &lt;script type=&quot;math/tex&quot;&gt;p_\phi(x \vert z)&lt;/script&gt;. The inference and generative networks have parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; respectively. The parameters are typically the weights and biases of the neural nets. We optimize these to maximize the ELBO using stochastic gradient descent (there are no global latent variables, so it is kosher to minibatch our data). We can write the ELBO and include the inference and generative network parameters as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ELBO_i(\theta, \phi) = E_{q_\theta(z\vert x_i)}[\log p_\phi(x_i\vert z)] - KL(q_\theta(z\vert x_i) \vert\vert p(z)).&lt;/script&gt;

&lt;p&gt;This evidence lower bound is the negative of the loss function for variational autoencoders we discussed from the neural net perspective; &lt;script type=&quot;math/tex&quot;&gt;ELBO_i(\theta, \phi) = -l_i(\theta, \phi)&lt;/script&gt;. However, we arrived at it from principled reasoning about probability models and approximate posterior inference. We can still interpret the Kullback-Leibler divergence term as a regularizer, and the expected likelihood term as a reconstruction ‘loss’. But the probability model approach makes clear why these terms exist: to minimize the Kullback-Leibler divergence between the approximate posterior &lt;script type=&quot;math/tex&quot;&gt;q_\lambda(z \vert x)&lt;/script&gt; and model posterior &lt;script type=&quot;math/tex&quot;&gt;p(z \vert x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;What about the model parameters? We glossed over this, but it is an important point. The term ‘variational inference’ usually refers to maximizing the ELBO with respect to the variational parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. We can also maximize the ELBO with respect to the model parameters &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; (e.g. the weights and biases of the generative neural network parameterizing the likelihood). This technique is called variational EM (expectation maximization), because we are maximizing the expected log-likelihood of the data with respect to the model parameters.&lt;/p&gt;

&lt;p&gt;That’s it! We have followed the recipe for variational inference. We’ve defined:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a probability model &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; of latent variables and data&lt;/li&gt;
  &lt;li&gt;a variational family &lt;script type=&quot;math/tex&quot;&gt;q&lt;/script&gt; for the latent variables to approximate our posterior&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then we used the variational inference algorithm to learn the variational parameters (gradient ascent on the ELBO to learn &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;). We used variational EM for the model parameters (gradient ascent on the ELBO to learn &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;).&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;Now we are ready to look at samples from the model. We have two choices to measure progress: sampling from the prior or the posterior. To give us a better idea of how to interpret the learned latent space, we can visualize what the posterior distribution of the latent variables &lt;script type=&quot;math/tex&quot;&gt;q_\lambda(z \vert x)&lt;/script&gt; looks like.&lt;/p&gt;

&lt;p&gt;Computationally, this means feeding an input image &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; through the inference network to get the parameters of the Normal distribution, then taking a sample of the latent variable &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. We can plot this during training to see how the inference network learns to better approximate the posterior distribution, and place the latent variables for the different classes of digits in different parts of the latent space. Note that at the start of training, the distribution of latent variables is close to the prior (a round blob around &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;).&lt;/p&gt;

&lt;center&gt;
&lt;iframe src=&quot;//giphy.com/embed/26ufoVqZDjHoPrp8k?html5=true&quot; width=&quot;480&quot; height=&quot;413&quot; frameborder=&quot;0&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;figure&gt;
    &lt;figcaption&gt;Visualizing the learned approximate posterior during training. As training progresses the digit classes become differentiated in the two-dimensional latent space. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We can also visualize the prior predictive distribution. We fix the values of the latent variables to be equally spaced between &lt;script type=&quot;math/tex&quot;&gt;-3&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;3&lt;/script&gt;. Then we can take samples from the likelihood parametrized by the generative network. These ‘hallucinated’ images show us what the model associates with each part of the latent space.&lt;/p&gt;

&lt;center&gt;
&lt;iframe src=&quot;//giphy.com/embed/26ufgj5LH3YKO1Zlu?html5=true&quot; width=&quot;480&quot; height=&quot;480&quot; frameborder=&quot;0&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;

&lt;figure&gt;
    &lt;figcaption&gt;Visualizing the prior predictive distribution by looking at samples of the likelihood. The x and y-axes represent equally spaced latent variable values between -3 and 3 (in two dimensions). &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;glossary&quot;&gt;&lt;a name=&quot;glossary&quot;&gt;&lt;/a&gt;Glossary&lt;/h3&gt;

&lt;p&gt;We need to decide on the language used for discussing variational autoencoders in a clear and concise way. Here is a glossary of terms I’ve found confusing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Variational Autoencoder (VAE)&lt;/strong&gt;: in neural net language, a VAE consists of an encoder, a decoder, and a loss function. In probability model terms, the variational autoencoder refers to approximate inference in a latent Gaussian model where the approximate posterior and model likelihood are parametrized by neural nets (the inference and generative networks).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Loss function&lt;/strong&gt;: in neural net language, we think of loss functions. Training means minimizing these loss functions. But in variational inference, we maximize the &lt;strong&gt;ELBO&lt;/strong&gt; (which is not a loss function). This leads to awkwardness like calling &lt;code&gt;optimizer.minimize(-elbo)&lt;/code&gt; as optimizers in neural net frameworks only support minimization.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt;: in the neural net world, the encoder is a neural network that outputs a representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; of data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;. In probability model terms, the &lt;strong&gt;inference network&lt;/strong&gt; parametrizes the approximate posterior of the latent variables &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. The inference network outputs parameters to the distribution &lt;script type=&quot;math/tex&quot;&gt;q(z \vert x)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt;: in deep learning, the decoder is a neural net that learns to reconstruct the data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; given a representation &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. In terms of probability models, the likelihood of the data &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; given latent variables &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; is parametrized by a &lt;strong&gt;generative network&lt;/strong&gt;. The generative network outputs parameters to the likelihood distribution &lt;script type=&quot;math/tex&quot;&gt;p(x \vert z)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Local latent variables&lt;/strong&gt;: these are the &lt;script type=&quot;math/tex&quot;&gt;z_i&lt;/script&gt; for each datapoint &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;. There are no global latent variables. Because there are only local latent variables, we can easily decompose the ELBO into terms &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}_i&lt;/script&gt; that depend only on a single datapoint &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;. This enables stochastic gradient descent.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: in neural nets, inference usually means prediction of latent representations given new, never-before-seen datapoints. In probability models, inference refers to inferring the values of latent variables given observed data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One jargon-laden concept deserves its own subsection:&lt;/p&gt;

&lt;h3 id=&quot;mean-field-versus-amortized-inference&quot;&gt;&lt;a name=&quot;mean-field&quot;&gt;&lt;/a&gt;Mean-field versus amortized inference&lt;/h3&gt;

&lt;!-- TODO: add plate diagrams for mean-field vs amortized inference --&gt;

&lt;p&gt;This issue was very confusing for me, and I can see how it might be even more confusing for someone coming from a deep learning background. In deep learning, we think of inputs and outputs, encoders and decoders, and loss functions. This can lead to fuzzy, imprecise concepts when learning about probabilistic modeling.&lt;/p&gt;

&lt;p&gt;Let’s discuss how mean-field inference differs from amortized inference. This is a choice we face when doing approximate inference to estimate a posterior distribution of latent variables. We might have various constraints: do we have lots of data? Do we have big computers or GPUs? Do we have local, per-datapoint latent variables, or global latent variables shared across all datapoints?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mean-field variational inference&lt;/strong&gt; refers to a choice of a variational distribution that factorizes across the &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; data points, with no shared parameters:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q(z) = \prod_i^{N} q(z_i; \lambda_i)&lt;/script&gt;

&lt;p&gt;This means there are free parameters for each datapoint &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; (e.g. &lt;script type=&quot;math/tex&quot;&gt;\lambda_i = (\mu_i, \sigma_i)&lt;/script&gt; for Gaussian latent variables). How do we do ‘learning’ for a new, unseen datapoint? We need to maximize the ELBO for each new datapoint, with respect to its mean-field parameter(s) &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Amortized inference&lt;/strong&gt; refers to ‘amortizing’ the cost of inference across datapoints. One way to do this is by sharing (amortizing) the variational parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; across datapoints. For example, in the variational autoencoder, the parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; of the inference network. These global parameters are shared across all datapoints. If we see a new datapoint and want to see what its approximate posterior &lt;script type=&quot;math/tex&quot;&gt;q(z_i)&lt;/script&gt; looks like, we can run variational inference again (maximizing the ELBO until convergence), or trust that the shared parameters are ‘good-enough’. This can be an advantage over mean-field.&lt;/p&gt;

&lt;p&gt;Which one is more flexible? Mean-field inference is strictly more expressive, because it has no shared parameters. The per-data parameters &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; can ensure our approximate posterior is most faithful to the data. Another way to think of this is that we are limiting the capacity or representational power of our variational family by tying parameters across datapoints (e.g. with a neural network that shares weights and biases across data).&lt;/p&gt;

&lt;h3 id=&quot;sample-implementation&quot;&gt;Sample implementation&lt;/h3&gt;

&lt;p&gt;Here is a simple implementation that was used to generate the figures in this post: &lt;a href=&quot;https://github.com/altosaar/vae/blob/master/vae.py&quot;&gt;Github link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;footnote-the-reparametrization-trick&quot;&gt;Footnote: the reparametrization trick&lt;/h3&gt;

&lt;p&gt;The final thing we need to implement the variational autoencoder is how to take derivatives with respect to the parameters of a stochastic variable. If we are given &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; that is drawn from a distribution &lt;script type=&quot;math/tex&quot;&gt;q_\theta (z \vert x)&lt;/script&gt;, and we want to take derivatives of a function of &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; with respect to &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, how do we do that? The &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; sample is fixed, but intuitively its derivative should be nonzero.&lt;/p&gt;

&lt;p&gt;For some distributions, it is possible to reparametrize samples in a clever way, such that the stochasticity is independent of the parameters. We want our samples to deterministically depend on the parameters of the distribution. For example, in a normally-distributed variable with mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and standard devation &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;, we can sample from it like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z = \mu + \sigma \odot \epsilon,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\epsilon \sim Normal(0, 1)&lt;/script&gt;. Going from &lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt; denoting a draw from the distribution to the equals sign &lt;script type=&quot;math/tex&quot;&gt;=&lt;/script&gt; is the crucial step. We have defined a function that depends on on the parameters deterministically. We can thus take derivatives of functions involving &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(z)&lt;/script&gt; with respect to the parameters of its distribution &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/images/reparametrization.png&quot; /&gt;
    &lt;figcaption&gt;The reparametrization trick allows us to push the randomness of a normally-distributed random variable z into epsilon, which is sampled from a standard normal. Diamonds indicate deterministic dependencies, circles indicate random variables. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In the variational autoencoder, the mean and variance are output by an inference network with parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; that we optimize. The reparametrization trick lets us backpropagate (take derivatives using the chain rule) with respect to &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; through the objective (the ELBO) which is a function of samples of the latent variables &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Is anything in this article confusing or can any explanation be improved? Please submit a &lt;a href=&quot;https://github.com/altosaar/jaan.io/blob/master/_posts/blog/2016-07-18-what-is-variational-autoencoder-vae-tutorial.md&quot;&gt;pull request&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/thejaan&quot;&gt;tweet me&lt;/a&gt;, or &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email me&lt;/a&gt; :)&lt;/p&gt;

&lt;h3 id=&quot;references-for-ideas-and-figures&quot;&gt;References for ideas and figures&lt;/h3&gt;

&lt;p&gt;Many ideas and figures are from Shakir Mohamed’s excellent blog posts on the &lt;a href=&quot;http://blog.shakirm.com/2015/10/machine-learning-trick-of-the-day-4-reparameterisation-tricks/&quot;&gt;reparametrization trick&lt;/a&gt; and &lt;a href=&quot;http://blog.shakirm.com/2015/03/a-statistical-view-of-deep-learning-ii-auto-encoders-and-free-energy/&quot;&gt;autoencoders&lt;/a&gt;.
Durk Kingma created the great visual of the &lt;a href=&quot;http://dpkingma.com/?page_id=277&quot;&gt;reparametrization trick&lt;/a&gt;. Great references for variational inference are this &lt;a href=&quot;https://arxiv.org/abs/1601.00670&quot;&gt;tutorial&lt;/a&gt; and David Blei’s &lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf&quot;&gt;course notes&lt;/a&gt;. Dustin Tran has a helpful blog post on &lt;a href=&quot;http://dustintran.com/blog/denoising-criterion-for-variational-auto-encoding-framework/&quot;&gt;variational autoencoders&lt;/a&gt;. The header’s MNIST gif is from &lt;a href=&quot;https://github.com/RuiShu/variational-autoencoder&quot;&gt;Rui Shu&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks to Rajesh Ranganath, Ben Poole, Jon Berliner, Cassandra Xia, and Ryan Sepassi for discussions and many concepts in this article.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/edit?id=12292576&quot;&gt;Hacker News&lt;/a&gt; and &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/4xv5b5/explainer_of_variational_autoencoders_from_a/&quot;&gt;Reddit&lt;/a&gt;. Featured in David Duvenaud’s course syllabus on &lt;a href=&quot;http://www.cs.toronto.edu/~duvenaud/courses/csc2541/&quot;&gt;“Differentiable inference and generative models”&lt;/a&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/what-is-variational-autoencoder-vae-tutorial/&quot;&gt;Tutorial - What is a variational autoencoder?&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on July 18, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Word embedding models for music analysis]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/word-embedding-music/" />
  <id>http://localhost:4000/word-embedding-music</id>
  <published>2016-07-02T00:00:00+02:00</published>
  <updated>2016-07-02T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;We use word2vec to analyze classical music and show that as composers use more dissonance, the principal components of chords on the circle of fifths become less circular.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.eamonnbell.com/&quot;&gt;Eamonn Bell&lt;/a&gt;, Jaan Altosaar. Word embedding models applied to classical music recover the circle of fifths in embedding space. &lt;a href=&quot;https://sites.google.com/site/ml4md2016/program&quot;&gt;ICML Music Discovery 2016&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/2016_Bell-Altosaar_word2vec-Music.pdf&quot;&gt;&lt;i class=&quot;fa fa-file-pdf-o&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt; &lt;a href=&quot;https://github.com/eamonnbell/music-mining&quot;&gt;&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt; Github&lt;/a&gt; &lt;a href=&quot;https://docs.google.com/presentation/d/1awRWuhEkkBlhk5NcloLtXKCDYkqdM-FdnPceXPP2XLk/edit?usp=sharing&quot;&gt;&lt;i class=&quot;fa fa-line-chart&quot;&gt;&lt;/i&gt; Slides&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/word-embedding-music/&quot;&gt;Word embedding models for music analysis&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on July 02, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Frustrated Spin Ice]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/spin-ice/" />
  <id>http://localhost:4000/spin-ice</id>
  <published>2016-01-01T00:00:00+01:00</published>
  <updated>2016-01-01T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;We studied a pyrochloric oxide and showed that it has quantum behavior, which is surprising as it is considered a material with only classical effects. &lt;em&gt;Featured&lt;/em&gt; on the &lt;a href=&quot;https://journals.aps.org/prb/kaleidoscope/prb/93/2/024402&quot;&gt;PRB front page&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;P. Henelius, T. Lin, M. Enjalran, Z. Hao, J. Altosaar, P. Henelius, F. Flicker, T. Yavors’kii, and M. J. P. Gingras. Refrustration and Competing Orders in a Spin Ice Material, &lt;a href=&quot;https://journals.aps.org/prb/kaleidoscope/prb/93/2/024402&quot;&gt;Phys. Rev. B. (2016)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/2015_Henelius-Lin-Enjalran-Hao-Rau-Altosaar-Flicker-Yavorskii-Gingras_Refrustration.pdf&quot;&gt;&lt;i class=&quot;fa fa-file-pdf-o&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt; &lt;a href=&quot;http://arxiv.org/abs/1512.05361&quot;&gt;&lt;i class=&quot;fa fa-institution&quot;&gt;&lt;/i&gt; arXiv&lt;/a&gt; &lt;a href=&quot;https://github.com/altosaar/CumulantExpander&quot;&gt;&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt; Github&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/spin-ice/&quot;&gt;Frustrated Spin Ice&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on January 01, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Experiments in information overload]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/info-overload/" />
  <id>http://localhost:4000/info-overload</id>
  <published>2015-11-27T00:00:00+01:00</published>
  <updated>2015-11-27T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;Why read clickbait over longform journalism? Why use Facebook so much?&lt;/p&gt;

&lt;p&gt;Screentime is bad, but the &lt;a href=&quot;http://www.tristanharris.com/essays/&quot;&gt;attention economy&lt;/a&gt; incentivizes our addiction.&lt;/p&gt;

&lt;p&gt;I’m trying to reduce information overload. Here are some methods I’ve used for a while.&lt;/p&gt;

&lt;p&gt;Thoughts on benefits, pitfalls, and other ideas? Hit me up at &lt;a href=&quot;mailto:j@jaan.io&quot;&gt;j@jaan.io&lt;/a&gt;. I’ll keep this updated.&lt;/p&gt;

&lt;p&gt;The current list:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;No news except for the Harper’s &lt;a href=&quot;http://harpers.org/blog/category/weekly-review/&quot;&gt;Weekly Review&lt;/a&gt; every Tuesday.&lt;/li&gt;
  &lt;li&gt;Two devices: a ‘work’ laptop and an ‘email/distraction’ device (Macbook Air and iPad).&lt;/li&gt;
  &lt;li&gt;On the work laptop, permanently block Gmail and any distracting sites (using &lt;a href=&quot;https://selfcontrolapp.com/&quot;&gt;SelfControl&lt;/a&gt; with &lt;a href=&quot;https://github.com/SelfControlApp/selfcontrol/wiki/Tweaking-Max-Block-Length-and-Block-Length-Interval&quot;&gt;extended block lengths&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Looking at &lt;a href=&quot;http://rescuetime.com/&quot;&gt;Rescuetime&lt;/a&gt; logs every week and updating my SelfControl blacklist with distracting websites.&lt;/li&gt;
  &lt;li&gt;No TV, and about five movies per year. If I’m forced to watch movies/TV/other media, watch it at 2x (it’s a bit harder to follow at first on these faster speeds; start at 1.5x and work your way up). Ditto for podcasts and lectures: 2x saves a lot of time with little loss in retention.&lt;/li&gt;
  &lt;li&gt;Every few months: skimming the top 20 posts of each day on &lt;a href=&quot;http://news.ycombinator.com/&quot;&gt;Hacker News&lt;/a&gt; using &lt;a href=&quot;http://hckrnews.com/&quot;&gt;HckrNews&lt;/a&gt;. Interesting links go to Instapaper. Doing this in batches helps filter useful things from hype cycle fare.&lt;/li&gt;
  &lt;li&gt;Once a year: skim RSS feeds using Feedly, save to Instapaper. I like &lt;a href=&quot;http://longform.org/&quot;&gt;Longform&lt;/a&gt;’s curation service of high quality long-form journalism and essays.&lt;/li&gt;
  &lt;li&gt;Many tricks from &lt;a href=&quot;https://medium.com/@jgvandehey/this-is-your-brain-on-mobile-15308056cfae&quot;&gt;‘This is your brain on mobile’&lt;/a&gt; like no phone notifications and no social media apps.&lt;/li&gt;
  &lt;li&gt;Using a pay-as-you-go &lt;a href=&quot;https://www.ptel.com/plans/pg&quot;&gt;phone plan&lt;/a&gt;. Combined with Google Voice, this is cheap and the 10c/MB data keeps me off mobile internet unless necessary (like maps when out).&lt;/li&gt;
  &lt;li&gt;On Facebook, systematically unfollowing everyone in the newsfeed (this took an hour of clicking, but was &lt;a href=&quot;http://www.newyorker.com/tech/elements/how-facebook-makes-us-unhappy&quot;&gt;well worth it&lt;/a&gt;). An empty newsfeed is revelatory: if I’m truly interested in someone, I’ll go to their profile page or use the graph search tool.&lt;/li&gt;
  &lt;li&gt;Leaving all devices at work as often as possible. Then I’m totally disconnected and forced to read, &lt;a href=&quot;http://chronicle.com/article/The-End-of-Solitude/3708&quot;&gt;be alone&lt;/a&gt;, and &lt;a href=&quot;http://www.nytimes.com/2014/08/10/opinion/sunday/hit-the-reset-button-in-your-brain.html&quot;&gt;take a real break&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;For complete blocking of ambient noise, 3M Peltor &lt;a href=&quot;http://www.amazon.com/gp/product/B00009LI4K/&quot;&gt;industrial earmuffs&lt;/a&gt; over in-ear &lt;a href=&quot;http://www.amazon.com/RBH-EP-2-Earphones/dp/B00H7LAJQA&quot;&gt;headphones&lt;/a&gt; give the most reduction I’ve found in crowded NYC subways.&lt;/li&gt;
  &lt;li&gt;Tara Brach’s &lt;a href=&quot;http://www.tarabrach.com/audioarchives-guided-meditations.html&quot;&gt;guided meditations&lt;/a&gt; and podcasts are awesome for &lt;a href=&quot;http://www.scientificamerican.com/article/mental-downtime/&quot;&gt;downtime&lt;/a&gt; and self-therapy during the week.&lt;/li&gt;
  &lt;li&gt;Making all desktop backgrounds pictures of your aged face (through an &lt;a href=&quot;http://faceretirement.merrilledge.com/&quot;&gt;app&lt;/a&gt;). This can add immediacy and help you make better decisions.&lt;/li&gt;
  &lt;li&gt;Using &lt;a href=&quot;https://chrome.google.com/webstore/detail/calendar-and-countdown/caplfhpahpkhhckglldpmdmjclabckhc?hl=en&quot;&gt;countdown apps&lt;/a&gt; to keep the number of &lt;em&gt;days&lt;/em&gt; to a big deadline visible every day. This can help connect your present self to your future self.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;benefits-and-pitfalls&quot;&gt;Benefits and pitfalls&lt;/h3&gt;

&lt;p&gt;When stuck I’m forced to go to lame sites (the interesting ones like Gmail and Hacker News are blocked). I can’t New Tab away from boredom and have to sit with the discomfort.&lt;/p&gt;

&lt;p&gt;I’m getting better staying with this existential fear-of-failure crisis of ‘I’m stuck’. Paying attention to the discomfort helps; eventually it dissipates and interesting ideas appear. But this doesn’t happen if I’m constantly checking emails, sites, or phone notifications.&lt;/p&gt;

&lt;p&gt;Severely limiting my information intake means I read more and get bored more often. It takes a few days to hear about big news sometimes.&lt;/p&gt;

&lt;p&gt;How could information filtering be applied to the physical world? I’d love to see the collections at the Met, MoMA, and other NYC museums, but there is no time. I need a recommendation system for this equivalent to Instapaper or Longform. Museum collections and physical things do not have a ‘save for later’ feature. Adblock for the real world isn’t here yet (c.f. augmented reality circa 2020). Meanwhile, can we mentally train to avoid advertising?&lt;/p&gt;

&lt;p&gt;If everyone used these weird hacks, no one would make money online and there would be no one to upvote things on Hacker News or Reddit; no one to ‘like’. These techniques might leave me more (or less) susceptible to &lt;a href=&quot;http://www.ted.com/talks/eli_pariser_beware_online_filter_bubbles&quot;&gt;filter bubbles&lt;/a&gt;, but it’s a fun experiment.&lt;/p&gt;

&lt;p&gt;Could recommendation systems be the solution to these issues? Would this accentuate the problem? Is there even a problem, or is this part of the &lt;a href=&quot;http://www.economist.com/news/christmas-specials/21636612-time-poverty-problem-partly-perception-and-partly-distribution-why&quot;&gt;‘busy’&lt;/a&gt; &lt;a href=&quot;http://opinionator.blogs.nytimes.com/2012/06/30/the-busy-trap/&quot;&gt;trap&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;Do let me know about alternative ideas or things to try at &lt;a href=&quot;mailto:j@jaan.io&quot;&gt;j@jaan.io&lt;/a&gt;!&lt;/p&gt;

&lt;h3 id=&quot;related-reading&quot;&gt;Related reading&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lareviewofbooks.org/article/entertain-yourself/#!&quot;&gt;Stuart Whatley&lt;/a&gt; on how smartphones cause anxiety and the psychology of boredom.&lt;/li&gt;
&lt;/ul&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/info-overload/&quot;&gt;Experiments in information overload&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on November 27, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[InferPy]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/inferpy/" />
  <id>http://localhost:4000/inferpy</id>
  <published>2015-11-26T00:00:00+01:00</published>
  <updated>2015-11-26T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;&lt;strong&gt;InferPy&lt;/strong&gt; is a high-level API for deep probabilistic modeling written in Python and capable of running on top of Edward and Tensorflow. InferPy’s API is strongly inspired by Keras and it has a focus on enabling flexible data processing, easy-to-code probablistic modelling and scalable inference.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/inferpy/&quot;&gt;InferPy&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on November 26, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Correlated LDA]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/correlated-LDA/" />
  <id>http://localhost:4000/correlated-LDA</id>
  <published>2015-09-09T00:00:00+02:00</published>
  <updated>2015-09-09T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;Goal: discover how articles in the humanities discuss concepts differently from the sciences.&lt;/p&gt;

&lt;p&gt;Jingwei Zhang, Aaron Gerow, Jaan Altosaar, James Evans, Richard Jean So. Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections, &lt;a href=&quot;https://www.cs.cmu.edu/~ark/EMNLP-2015/index.html&quot;&gt;EMNLP 2015&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/2015_Zhang-Gerow-Altosaar-Evans-So_Correlated-LDA.pdf&quot;&gt;&lt;i class=&quot;fa fa-file-pdf-o&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt; &lt;a href=&quot;http://arxiv.org/abs/1508.04562&quot;&gt;&lt;i class=&quot;fa fa-institution&quot;&gt;&lt;/i&gt; arXiv&lt;/a&gt; &lt;a href=&quot;https://github.com/iceboal/correlated-lda&quot;&gt;&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt; Github&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/correlated-LDA/&quot;&gt;Correlated LDA&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on September 09, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[MusicMappr]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/mapping-music/" />
  <id>http://localhost:4000/mapping-music</id>
  <published>2014-12-28T00:00:00+01:00</published>
  <updated>2014-12-28T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;Enable anyone to make a beat in 30 seconds, using machine learning in JavaScript. &lt;em&gt;Press!&lt;/em&gt; Interviewed and live demo featured on &lt;a href=&quot;http://www.thewire.co.uk/in-writing/interviews/play-the-musicmappr-sampling-app&quot;&gt;The Wire&lt;/a&gt; magazine!&lt;/p&gt;

&lt;p&gt;Ethan Benjamin, Jaan Altosaar. MusicMapper: Interactive 2D representations of music samples for in-browser remixing and exploration, &lt;a href=&quot;http://www.nime.org/wp-publications/jaltosaar2015/&quot;&gt;NIME 2015&lt;/a&gt;, Louisiana State University, 2015.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/2015_Benjamin-Altosaar_MusicMapper.pdf&quot;&gt;&lt;i class=&quot;fa fa-file-pdf-o&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;  &lt;a href=&quot;https://github.com/fatsmcgee/MusicMappr&quot;&gt;&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt; Github&lt;/a&gt;  &lt;a href=&quot;https://www.youtube.com/watch?v=mvD6e1uiO8k&quot;&gt;&lt;i class=&quot;fa fa-youtube-play&quot;&gt;&lt;/i&gt; YouTube demo&lt;/a&gt;  &lt;a href=&quot;http://fatsmcgee.github.io/MusicMappr/&quot;&gt;&lt;i class=&quot;fa fa-laptop&quot;&gt;&lt;/i&gt; Live demo&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/mapping-music/&quot;&gt;MusicMappr&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on December 28, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Fish Music]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/fish-music/" />
  <id>http://localhost:4000/fish-music</id>
  <published>2014-12-28T00:00:00+01:00</published>
  <updated>2014-12-28T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;Can we convey how fish move using generative music and computer vision?&lt;/p&gt;

&lt;p&gt;Andrew Mercer-Taylor, Jaan Altosaar. Sonification of Fish Movement Using Pitch Mesh Pairs, New Interfaces For Musical Expression, Louisiana State University, 2015.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/papers/2015_Mercer-Taylor-Altosaar_Fish-Music.pdf&quot;&gt;&lt;i class=&quot;fa fa-file-pdf-o&quot;&gt;&lt;/i&gt; PDF&lt;/a&gt;  &lt;a href=&quot;https://github.com/andrewjmt/fishmusic&quot;&gt;&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt; Github&lt;/a&gt;  &lt;a href=&quot;https://www.youtube.com/watch?v=HzsFGQyIpuc&quot;&gt;&lt;i class=&quot;fa fa-youtube-play&quot;&gt;&lt;/i&gt; YouTube demo&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/fish-music/&quot;&gt;Fish Music&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on December 28, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Princeton Pianos]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/princeton-pianos/" />
  <id>http://localhost:4000/princeton-pianos</id>
  <published>2014-01-18T00:00:00+01:00</published>
  <updated>2014-01-18T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;I’ve played more piano since starting grad school than throughout my four years at McGill, thanks to the abundance of pianos on campus. The pianos’ conditions range from excellent to passable, with some sporting &lt;a href=&quot;http://en.wikipedia.org/wiki/The_K%C3%B6ln_Concert#The_K.C3.B6ln_concert&quot;&gt;Köln&lt;/a&gt;-level limitations. However, I couldn’t find a resource listing their locations.&lt;/p&gt;

&lt;p&gt;Another ‘open-sourced locations’ project could be to document uncommon study spots. For example, if McGill libraries were crowded I could escape to Purvis Hall’s &lt;a href=&quot;https://www.google.com/maps?ll=45.50447900000001%2C-73.58179800000002&amp;amp;cbp=%2C70.62%2C%2C2%2C-4.739998&amp;amp;layer=c&amp;amp;panoid=tTI0x7ujrMYKWjVhRAd7lw&amp;amp;spn=0.18000000000000788%2C0.30000000000001953&amp;amp;output=classic&amp;amp;cbll=45.504479%2C-73.581798&quot;&gt;solarium&lt;/a&gt;, which was usually empty.&lt;/p&gt;

&lt;p&gt;If I missed a piano, &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;shoot me an email&lt;/a&gt; and I will add it to the map (&lt;a href=&quot;https://mapsengine.google.com/map/edit?mid=zZ0UKQQpeAC0.kCaCPXTnIEOo&quot;&gt;mobile-friendly link&lt;/a&gt;):&lt;/p&gt;

&lt;iframe src=&quot;https://mapsengine.google.com/map/embed?mid=zZ0UKQQpeAC0.kCaCPXTnIEOo&quot; width=&quot;900&quot; height=&quot;600&quot;&gt;&lt;/iframe&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/pianochapel.jpg&quot; /&gt;
	&lt;figcaption&gt;Piano D: The Steinway in the lecture room in the basement of the chapel.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/carillon.jpg&quot; /&gt;
	&lt;figcaption&gt;'Piano' a: Technically not a piano, the carillon is a unique instrument that's close enough!&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/pianomaddy.jpg&quot; /&gt;
	&lt;figcaption&gt;Piano N: The Yamaha in the Mathey college common room.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Thanks to Jordan Ash for his camera-lending abilities.&lt;/em&gt;&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/princeton-pianos/&quot;&gt;Princeton Pianos&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on January 18, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[AMIDST]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/amidst/" />
  <id>http://localhost:4000/amidst</id>
  <published>2013-10-06T00:00:00+02:00</published>
  <updated>2013-10-06T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;The &lt;strong&gt;AMIDST Toolbox&lt;/strong&gt; is an open source Java software for scalable probabilistic machine learning with a special focus on (massive) streaming data. The toolbox supports a flexible modelling language based on probabilistic graphical models with latent variables.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/amidst/&quot;&gt;AMIDST&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on October 06, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[How to apply to grad school]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/how-to-apply-to-grad-school/" />
  <id>http://localhost:4000/how-to-apply-to-grad-school</id>
  <updated>2015-10-23T00:00:00-00:00</updated>
  <published>2013-09-01T00:00:00+02:00</published>
  
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;I gave an info session at McGill in March 2013 on applying to graduate schools in Canada, the U.S., and Britain. Here are some things I found most useful and what I wish I had known:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What is grad school? &lt;a href=&quot;http://matt.might.net/articles/phd-school-in-pictures/&quot;&gt;Matt Might&lt;/a&gt; might have the answer.&lt;/li&gt;
  &lt;li&gt;Decide where to apply: talk to professors at your school in the fields you are interested in and ask them what schools have the best research programs and people in those fields. If you have multiple interests or haven’t decided what you want to study, pick schools with research groups in a variety of fields.&lt;/li&gt;
  &lt;li&gt;Deadlines: write down every deadline of every school and fellowship you are considering. If you are applying to start in September, there will be deadlines as early as September the previous year, meaning you should aim to get those applications &lt;em&gt;finished&lt;/em&gt; in August of the previous year. It is never too early to write down the deadlines of every school and fellowship you are applying for (e.g. the Rhodes, NSERC PGS, and Vanier deadlines are in September; Oxford’s first deadline is in October).&lt;/li&gt;
  &lt;li&gt;Requirements: figure out what you need to submit to the school to apply (viz. navigate the mazes of academic websites). A typical application consists of three letters of recommendation, a two-page statement of purpose (essay), CV, an application fee, and test scores (GRE, subject GRE, and TOEFL).&lt;/li&gt;
  &lt;li&gt;Apply to as many schools as you can - once you have one application done, additional applications don’t take much time. People typically apply to around ten schools (a few top schools, a few in the middle, and a few ‘safeties’ where admission is anticipated).&lt;/li&gt;
  &lt;li&gt;Do not worry about the cost of tests and applications - most programs will pay you a decent salary and you will readily make back what you spent (if you gain admission to just one program).&lt;/li&gt;
  &lt;li&gt;Apply to every scholarship and fellowship you are eligible for which will support your graduate studies. It is good practice writing the research statements and essays and you may even be successful.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-application&quot;&gt;The Application&lt;/h3&gt;

&lt;p&gt;Your application will be reviewed by a committee of faculty members (and sometimes senior graduate students). The majority of your graduate schooling consists of doing research – the most important thing you can do in your application is to demonstrate research ability. The best way to do this is to do summer research and work hard in the hopes of getting published and getting good recommendation letters attesting to your research potential. The next best way is to take research-based courses at your school.&lt;/p&gt;

&lt;p&gt;Therefore, start doing research as early as possible. If you’re in high school, send me an &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email&lt;/a&gt; and I’ll try to give you a possible path to working in a lab. If you are in college:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Start talking to potential summer research supervisors in November of the previous year (in January of the same year at the latest). You can also apply to work at different schools - a bit harder, but could happen through same process. Look up professors you are interested in working with, and send them a short email with your LaTeXed CV (see below) asking about doing a summer project and in the area of their research that appeals to you. Read one of their papers and mention it in your email (their website may not be current; look up their latest papers on &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed&quot;&gt;PubMed&lt;/a&gt;, the &lt;a href=&quot;http://arxiv.org/&quot;&gt;arXiv&lt;/a&gt;, or the &lt;a href=&quot;http://www.webofknowledge.com/&quot;&gt;ISI Web of Knowledge&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Apply for an REU if you are a US citizen, or an &lt;a href=&quot;http://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp&quot;&gt;NSERC USRA&lt;/a&gt; if you are Canadian or to the Caltech SURF program if you are either or neither. The &lt;a href=&quot;https://www.daad.de/rise/en/&quot;&gt;DAAD RISE&lt;/a&gt; program has internship opportunities in Germany
Jan Gorzny at UToronto has a &lt;a href=&quot;http://www.crypticcode.ca/jan.gorzny/2011/05/nserc-usra-advice/&quot;&gt;helpful page&lt;/a&gt; on the NSERC USRA.&lt;/li&gt;
  &lt;li&gt;Apply for every summer research scholarship such as the REU, DAAD RISE, Caltech SURF, or NSERC USRA even if you do not have the most competitive application and transcript, as the professor you apply with may decide to fund you through a separate grant if your initial funding application is not successful
You may have to contact many professors before you find one willing to take you on - this is normal (I contacted around 30 faculty a year and the success rate was ~10%).&lt;/li&gt;
  &lt;li&gt;Take research courses, as electives or for your degree. For these you will also have to seek out professors to work with. At McGill such courses are the &lt;a href=&quot;http://www.mcgill.ca/science/research/ours/396&quot;&gt;396 research courses&lt;/a&gt;, and other possible routes are MATH 470 (Honours Research Project) or PHYS 459 (Honours Research Project or Thesis).&lt;/li&gt;
  &lt;li&gt;Before contacting professors, read Matt Might’s &lt;a href=&quot;http://matt.might.net/articles/how-to-email/&quot;&gt;how to email post&lt;/a&gt;, use your official school email address, and &lt;a href=&quot;http://www.boomeranggmail.com/&quot;&gt;Boomerang&lt;/a&gt; your emails to arrive at 3 PM on Wednesdays (see MailChimp’s &lt;a href=&quot;http://mailchimp.com/resources/research/&quot;&gt;email open rates summary&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;If you cannot find a professor willing to take you on for the summer, consider volunteering in a lab for a few hours each week or take a research course to get your foot in the door. If you have LaTeXed your CV (see below), tried the above options, contacted a ton of professors, and have been unsuccessful in securing a summer or semester-long position, send me an &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email&lt;/a&gt; and I will do my best to tell you how to improve your application.&lt;/li&gt;
  &lt;li&gt;Persistence pays off with professors - if they don’t reply initially, show up at their office or send a follow up email. You can also attend local colloquia or talks in fields that interest you; approach professors after their talk to ask about opportunities at their school, get their card or contact info, and follow up via email.&lt;/li&gt;
  &lt;li&gt;Once you’re working in research, do your best to see your project through from start to finish (this may mean putting in extra, unpaid time).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;making-your-cv-look-good&quot;&gt;Making your CV look good&lt;/h3&gt;

&lt;p&gt;When contacting professors with your CV, make sure it looks good - presentation makes a difference. Don’t use Microsoft Word. Don’t believe me that you should use LaTeX for your CV? Read &lt;a href=&quot;http://nitens.org/taraborelli/latex&quot;&gt;this&lt;/a&gt; for an overview of the benefits.&lt;/p&gt;

&lt;p&gt;To get your CV into LaTeX format, you can look online for CV templates - a good website is &lt;a href=&quot;http://www.latextemplates.com/cat/curricula-vitae&quot;&gt;LaTeX Templates&lt;/a&gt;. Mike King also has a &lt;a href=&quot;http://michaelelliotking.com/articles/learn_latex/&quot;&gt;good intro&lt;/a&gt; to LaTeX.&lt;/p&gt;

&lt;h3 id=&quot;hosting-your-cv-setting-up-a-website&quot;&gt;Hosting your CV, setting up a website&lt;/h3&gt;

&lt;p&gt;Consider setting up a basic website with your CV and projects. You can do this with Google Sites or Wordpress (or Jekyll if you are comfortable with the command line).&lt;/p&gt;

&lt;p&gt;At the very least, include a &lt;a href=&quot;https://www.dropbox.com/help/167/en&quot;&gt;Dropbox link&lt;/a&gt; to your CV whenever you send it in an email. This way you can update your CV at any time and rest assured that the recipients will see the latest version.&lt;/p&gt;

&lt;h3 id=&quot;studying-for-the-gre-and-subject-gre&quot;&gt;Studying for the GRE and subject GRE&lt;/h3&gt;

&lt;p&gt;See &lt;a href=&quot;https://jaan.io/how-to-ace-the-gre-and-physics-gre&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-personal-statement&quot;&gt;The Personal Statement&lt;/h3&gt;

&lt;p&gt;Read the guidelines for each school you are applying to - while typically they will ask you to elaborate on your research projects, courses, and future plans, some may ask about teaching or other specific things. If you are applying to many schools, you can use the same essay but change your ‘future plans’ section appropriately. Some tips:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Even if you are not sure of what field you are interested in, pick something that sounds interesting and fits your background and stick to it. If you are deciding between theory and experiment, pick experiment, as it is very difficult to get accepted for theory these days, and most end up switching to experiment anyway. The most convincing essay will typically be the one where you appear most sure of what you want to study.&lt;/li&gt;
  &lt;li&gt;You are not bound by your statement - you will typically do rotations in three to four groups before deciding on an advisor.&lt;/li&gt;
  &lt;li&gt;Write down the names of a few professors at the school you are writing the statement for; they will typically be the ones reviewing your case. Make sure your background matches their research program, and that you state which aspect of their research you are interested in.&lt;/li&gt;
  &lt;li&gt;Write as many concrete examples of projects you did, or things that make you stand out. Have any motivation be as concise as possible (e.g. avoid the ubiquitous “Ever since grade school I knew I wanted to study [insert subject here].”)&lt;/li&gt;
  &lt;li&gt;Here are two sample essays:
    &lt;ul&gt;
      &lt;li&gt;DJ Strouse’s &lt;a href=&quot;http://djstrouse.com/downloads/UWash_Physics-Statement_of_Purpose-DJ_Strouse-1-page.pdf&quot;&gt;one page&lt;/a&gt; and &lt;a href=&quot;http://djstrouse.com/downloads/UWash_Physics-Statement_of_Purpose-DJ_Strouse-2-page.pdf&quot;&gt;two page&lt;/a&gt; statements.&lt;/li&gt;
      &lt;li&gt;Anon’s &lt;a href=&quot;/files/anon_harvard_statement.pdf&quot;&gt;two page&lt;/a&gt; statement of purpose for Harvard.&lt;/li&gt;
      &lt;li&gt;My &lt;a href=&quot;/files/JaanAltosaar_Princeton_PersonalStatement.pdf&quot;&gt;two page&lt;/a&gt; statement of purpose.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;letters-of-recommendation&quot;&gt;Letters of Recommendation&lt;/h3&gt;

&lt;p&gt;Matt Might has &lt;a href=&quot;http://matt.might.net/articles/how-to-recommendation-letter/&quot;&gt;good advice&lt;/a&gt; on this, as does &lt;a href=&quot;http://chrisblattman.com/advising/letters/&quot;&gt;Chris Blattman&lt;/a&gt;. Alex Maloney’s &lt;a href=&quot;http://www.physics.mcgill.ca/~maloney/letter_instructions.pdf&quot;&gt;instructions&lt;/a&gt; are good and would apply to any professor you request letters of recommendations from.&lt;/p&gt;

&lt;h3 id=&quot;scholarships-and-fellowships&quot;&gt;Scholarships and Fellowships&lt;/h3&gt;

&lt;p&gt;Apply to every scholarship and fellowship for which you are eligible. For me, this included the &lt;a href=&quot;http://www.rhodeshouse.ox.ac.uk/rhodesscholarship&quot;&gt;Rhodes Scholarship&lt;/a&gt; (I strongly encourage you to apply: the interviews are nerve-wracking and great practice), &lt;a href=&quot;http://cscuk.dfid.gov.uk/apply/scholarships-developed-cw/&quot;&gt;Commonwealth Scholarship&lt;/a&gt;, &lt;a href=&quot;http://www.nserc-crsng.gc.ca/Students-Etudiants/PG-CS/index_eng.asp&quot;&gt;NSERC PGSM&lt;/a&gt;, &lt;a href=&quot;http://www.vanier.gc.ca/eng/home-accueil.aspx&quot;&gt;Vanier Canada Scholarship&lt;/a&gt;, &lt;a href=&quot;http://www.fulbright.ca/programs/canadian-students/traditional-awards.html&quot;&gt;Fulbright Scholarship&lt;/a&gt;, &lt;a href=&quot;http://www.mkingscholarships.ca/index-e.html&quot;&gt;Mackenzie Scholarship&lt;/a&gt; (&lt;a href=&quot;http://www.mcgill.ca/gps/funding/students-postdocs/students/mackenzie&quot;&gt;McGill link&lt;/a&gt;), &lt;a href=&quot;http://www.mcgill.ca/gps/funding/students-postdocs/students/mackenzie&quot;&gt;Delta Upsilon Scholarship&lt;/a&gt; (McGill only), and &lt;a href=&quot;http://www.mcgill.ca/science/student/moyse/&quot;&gt;Moyse Travelling Scholarship&lt;/a&gt; (McGill only). Lesser known sources of scholarships that may have many fewer applicants are offered through professional organizations such as the SPIE, IEE, etc. (see below).&lt;/p&gt;

&lt;h3 id=&quot;attend-conferences-try-a-semester-abroad-join-professional-associations&quot;&gt;Attend conferences, try a semester abroad, join professional associations&lt;/h3&gt;

&lt;p&gt;If you have done research, make a poster and present it at a conference or meeting, regardless of whether you confirmed your hypothesis. Conferences typically have funding you can apply for, and your school may have funds like &lt;a href=&quot;http://sus.mcgill.ca/Menu-Documents/Ambassador-Fund-Application.pdf&quot;&gt;McGill’s Ambassador Fund&lt;/a&gt; to enable students to attend conferences. Additional sources of funding include professional associations, which typically provide free membership for undergraduates. Examples are the &lt;a href=&quot;http://www.cap.ca/&quot;&gt;Canadian Association of Physicists&lt;/a&gt;, &lt;a href=&quot;http://www.iop.org/&quot;&gt;Institute of Physics (IOP)&lt;/a&gt;, &lt;a href=&quot;http://spie.org/&quot;&gt;SPIE&lt;/a&gt;, &lt;a href=&quot;http://www.ieee.org/&quot;&gt;IEEE&lt;/a&gt;, and &lt;a href=&quot;http://societyofwomenengineers.swe.org/&quot;&gt;Society of Women Engineers&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Canadian Association of Physicists provides funding for the &lt;a href=&quot;http://cupc.ca/&quot;&gt;Canadian Undergraduate Physics Conference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://www.aps.org/programs/women/workshops/cuwip.cfm&quot;&gt;Conferences for Undergraduate Women in Physics&lt;/a&gt; provides funding to attend the yearly conference&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://cumc.math.ca/&quot;&gt;Canadian Undergraduate Mathematical Conference&lt;/a&gt; also has funding opportunities&lt;/li&gt;
  &lt;li&gt;The King Abdullah University of Science and Technology holds an &lt;a href=&quot;http://www.kaust.edu.sa/academics/wep/&quot;&gt;undergraduate research poster competition&lt;/a&gt; to which you can apply for full funding to attend&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://www.killamfellowships.com/programs.html&quot;&gt;Killam Fellowship&lt;/a&gt; provides funding for a semester abroad in the US&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contact-professors-before-and-after-applying&quot;&gt;Contact professors before (and after) applying&lt;/h3&gt;

&lt;p&gt;Send emails to the professors you are interested in working with at the schools you are applying to, ideally well before actually applying. This serves several purposes: you can find out if they will be taking new students or not (this is important - if you only list faculty who are not taking students on your personal statement you are likely to be rejected). Furthermore, you will be able to include your CV if it is not asked for on the official application. To do this, read or skim their latest papers from their website, PubMed, the arXiv, or the ISI Web of Knowledge, and mention which areas of their research interest you. Read Matt Might’s &lt;a href=&quot;http://matt.might.net/articles/how-to-email/&quot;&gt;how to email&lt;/a&gt;, use your official school email address, add a direct link to your hosted CV (see above), and Boomerang your emails to arrive at 9 AM on a Wednesday. Getting personal replies from profs makes a huge difference and can make the process feel much more personal (as well as being good motivation to grind through the months).&lt;/p&gt;

&lt;h3 id=&quot;your-final-year-grades-dont-matter-that-much-and-some-good-courses-to-take&quot;&gt;Your final-year grades don’t matter that much (and some good courses to take)&lt;/h3&gt;

&lt;p&gt;Caveat: they do matter if you plan on doing a Masters and then applying to PhD programs or possibly working in industry, and it’s never bad to have a high GPA.&lt;/p&gt;

&lt;p&gt;However, between courses, GREs, and applications it is easy to get burnt out, so don’t sweat your grades if you get overwhelmed.  Try to plan your courses to maximize your grades (GPA) for the first three years of your undergrad, as grad schools will not see the fall semester grades of your senior year (you apply in December and grades come out in January by which time you will already be hearing back). Alongside research courses, try doing your undergrad thesis in your third year even if it’s normally taken in your senior year (if you work hard you’ll have a publication to list on your application, a good letter of recommendation, and some A’s). Also consider taking scientific writing courses such as McGill’s &lt;a href=&quot;http://www.mcgill.ca/study/2012-2013/courses/ceap-250&quot;&gt;CEAP 250 course&lt;/a&gt; - these will improve your writing and typically culminate in a final report which you could also submit for publication.&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further reading&lt;/h3&gt;

&lt;p&gt;Other webpages and resources I found useful:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Matt Might’s &lt;a href=&quot;http://matt.might.net/articles/how-to-apply-and-get-in-to-graduate-school-in-science-mathematics-engineering-or-computer-science/&quot;&gt;super-useful advice&lt;/a&gt; on applying (he has been on admissions committees) and &lt;a href=&quot;http://matt.might.net/articles/college-tips/&quot;&gt;general college tips&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sean Carroll’s advice on grad school &lt;a href=&quot;http://blogs.discovermagazine.com/cosmicvariance/2005/12/20/unsolicited-advice-1-how-to-get-into-graduate-school/&quot;&gt;part one&lt;/a&gt; and &lt;a href=&quot;http://blogs.discovermagazine.com/cosmicvariance/2006/03/29/unsolicited-advice-part-deux-choosing-a-grad-school/&quot;&gt;part two&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;An &lt;a href=&quot;http://blogs.discovermagazine.com/cosmicvariance/2008/01/29/the-other-side-of-graduate-admissions/&quot;&gt;inside look at admissions&lt;/a&gt; from a former committee member&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.amazon.com/gp/product/0307591549/&quot;&gt;The Happiness Advantage&lt;/a&gt; by Shawn Achor is a fantastic summary of recent positive psychology research for staying sane through this entire process&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.amazon.com/gp/product/0465022227/&quot;&gt;A PhD is not enough&lt;/a&gt; gives a great overview of what grad school is like and what to expect afterwards&lt;/li&gt;
  &lt;li&gt;DJ Strouse’s excellent &lt;a href=&quot;http://djstrouse.com/guide-to-applying-to-us-science-phd-programs-and-fellowships/&quot;&gt;all-encompassing guide&lt;/a&gt; to applying to grad schools&lt;/li&gt;
  &lt;li&gt;Philip Guo’s &lt;a href=&quot;http://www.pgbovine.net/PhD-memoir.htm&quot;&gt;PhD Grind&lt;/a&gt; gives a realistic look at grad school&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-did-i-miss&quot;&gt;What did I miss?&lt;/h3&gt;

&lt;p&gt;Shoot me an &lt;a href=&quot;mailto:altosaar@princeton.edu&quot;&gt;email&lt;/a&gt; with any recommendations or tips.&lt;/p&gt;


  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/how-to-apply-to-grad-school/&quot;&gt;How to apply to grad school&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on September 01, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[How to ace the GRE and Physics GRE]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/how-to-ace-the-gre-and-physics-gre/" />
  <id>http://localhost:4000/how-to-ace-the-gre-and-physics-gre</id>
  <updated>2013-08-28T00:00:00-00:00</updated>
  <published>2013-08-31T00:00:00+02:00</published>
  
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;Most PhD programs in the US require the GRE general test. These scores matter, so start studying early and register at ETS to take the tests as early as possible.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For the general GRE test, I used and highly recommend The Princeton Review’s Cracking the GRE 2013 edition. It is excellent preparation and includes plenty of practice tests.&lt;/li&gt;
  &lt;li&gt;Don’t get an older version (e.g. the 2012 version), but get the &lt;a href=&quot;http://www.amazon.com/Cracking-Practice-Edition-Graduate-Preparation/dp/0307945634&quot;&gt;2014 edition&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;ETS periodically adds new material to the tests so you want the most up-to-date book. You can take a computer-based or hand-written version of the test; I recommend the handwritten version as it is easier to make notes and work out the problems on the test booklet itself than on paper beside the computer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.ets.org/gre/subject/about/content/physics&quot;&gt;Physics GRE subject test&lt;/a&gt; is much more involved, and much more difficult. There are no omnipotent books from The Princeton Review, and you have to do the best you can using a variety of sources. Your Physics GRE score matters a lot if you are applying to Physics programs (especially if you are an international student, as some schools have cutoffs). Successful preparation in one sentence would be: do the 500 practice problems found online (links below) and understand the solutions and how to do them as fast as possible. In summary:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Your goal should not necessarily be to understand the material&lt;/strong&gt;; it should be to ace this test in as little time as possible (not even ace – there is a heavy curve so an excellent score is typically 80 questions correct out of 100).&lt;/li&gt;
  &lt;li&gt;You have &lt;strong&gt;very little time per question&lt;/strong&gt; so once you’ve understood a question, check the solutions websites if a comment gives a faster method of solving it by taking limits or physical intuition. Memorize the formulas you see popping up again and again (see formula sheets below).&lt;/li&gt;
  &lt;li&gt;Focus all your effort on writing the practice tests and reviewing the questions. Do not rely heavily or spend much time (if any) on an official ‘How to prepare for the Physics GRE’ book – the material is too extensive to be condensed into this form.
The 2012 Physics GRE was very similar to the 2008 test, so it pays to ensure you understand and can quickly do all 500 practice questions. This will take time so start early.&lt;/li&gt;
  &lt;li&gt;Preferably take the April test (register &lt;a href=&quot;http://www.ets.org/gre/subject/about/content/physics&quot;&gt;here&lt;/a&gt;) so you can write the October test if you need to (if you write the November test you will not have a chance to retake it, and if you take the October test you will not receive your scores by the November test).&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The old tests from the 90s are much harder than the current versions, so if you are short on time focus more on the most recent practice tests.
Here are links to the five practice tests found online, with online solutions. &lt;strong&gt;IMPORTANT: read the comments of the online solutions, as they frequently give ways of solving problems much faster.&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.ets.org/s/gre/pdf/practice_book_physics.pdf&quot;&gt;GR0877&lt;/a&gt; &lt;a href=&quot;http://physicsworks.wordpress.com/2011/07/16/gr0877-solutions/&quot;&gt;(online solutions)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.physics.ohio-state.edu/undergrad/greStuff/exam_GR0177.pdf&quot;&gt;GR0177&lt;/a&gt; &lt;a href=&quot;http://grephysics.net/ans/all-solutions_list.php&quot;&gt;(online solutions)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.physics.ohio-state.edu/undergrad/greStuff/exam_GR9677.pdf&quot;&gt;GR9677&lt;/a&gt; &lt;a href=&quot;http://grephysics.net/ans/all-solutions_list.php&quot;&gt;(online solutions)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.physics.ohio-state.edu/undergrad/greStuff/exam_GR9277.pdf&quot;&gt;GR9277&lt;/a&gt; &lt;a href=&quot;http://grephysics.net/ans/all-solutions_list.php&quot;&gt;(online solutions)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.physics.ohio-state.edu/undergrad/greStuff/exam_GR8677.pdf&quot;&gt;GR8677&lt;/a&gt; &lt;a href=&quot;http://grephysics.net/ans/all-solutions_list.php&quot;&gt;(online solutions)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The awesome folks at Case Western Reserve University will send you &lt;strong&gt;free (FREE!) flash cards&lt;/strong&gt; for the Physics GRE, all you have to do is send your address to &lt;a href=&quot;mailto:physicsgreflashcards@phys.cwru.edu&quot;&gt;physicsgreflashcards@phys.cwru.edu&lt;/a&gt; and they will put them in the mail (also have a look at their &lt;a href=&quot;http://www.phys.cwru.edu/flashCards/&quot;&gt;website&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Two former MIT students wrote a &lt;a href=&quot;http://www.amazon.com/gp/product/1479274631/&quot;&gt;decent book&lt;/a&gt; attempting to summarize the material (I used a few chapters of this to review material).&lt;/li&gt;
  &lt;li&gt;Here are links to some formula sheets:
    &lt;ul&gt;
      &lt;li&gt;Steven J. Byrnes of Harvard wrote a helpful and  up-to-date &lt;a href=&quot;http://sjbyrnes.com/studysheet.pdf&quot;&gt;formula sheet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;The Harvard Society of Physics Students also hosts &lt;a href=&quot;http://www.hcs.harvard.edu/~physics/wp-content/uploads/2013/02/GRE-notes.pdf&quot;&gt;some formula sheets&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Additional ‘how to prepare for the Physics GRE pages’:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.hcs.harvard.edu/~physics/gre-resources/&quot;&gt;GRE Resources&lt;/a&gt; from the Harvard Society of Physics Students&lt;/li&gt;
      &lt;li&gt;The infamous &lt;a href=&quot;http://www.physicsgre.com/&quot;&gt;PhysicsGRE.com forum&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Alex Lang of BU has a &lt;a href=&quot;http://www.alexhunterlang.com/physics-gre&quot;&gt;Physics GRE advice&lt;/a&gt; page with a study plan&lt;/li&gt;
      &lt;li&gt;DJ Strouse of Princeton also has &lt;a href=&quot;http://djstrouse.com/guide-to-applying-to-us-science-phd-programs-and-fellowships/&quot;&gt;GRE advice&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://blogs.discovermagazine.com/cosmicvariance/2009/10/22/an-inside-look-at-the-physics-gre/&quot;&gt;An inside look at the Physics GRE&lt;/a&gt; from a former testing committee member&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;


  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/how-to-ace-the-gre-and-physics-gre/&quot;&gt;How to ace the GRE and Physics GRE&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on August 31, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Elvira]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/elvira/" />
  <id>http://localhost:4000/elvira</id>
  <published>2013-08-15T00:00:00+02:00</published>
  <updated>2013-08-15T00:00:00+02:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;The &lt;strong&gt;Elvira&lt;/strong&gt; system is a Java tool to construct probabilistic models-driven based decision support systems. Elvira works with Bayesian networks and influence diagrams and it can operate with discrete, continuous and temporal variables. It has an easy to use Graphical User Interface (GUI).&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/elvira/&quot;&gt;Elvira&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on August 15, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Smoked salmon openfacer]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/smoked-salmon-open-faced-sandwich.md/" />
  <id>http://localhost:4000/smoked-salmon-open-faced-sandwich.md</id>
  <updated>2015-11-24T00:00:00-00:00</updated>
  <published>2013-07-19T00:00:00+02:00</published>
  
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">This sandwich is an all-time classic hall-of-famer mainstay in my family. It's a classy breakfast, lunch, or dinner, and is great for hosting (guests can make their own sandwiches). Just brush your teeth after consumption.

###Ingredients

* White bread with sesame seeds, well-toasted
* Unsalted butter
* Fresh smoked salmon, sliced [[^1]]
* Lemon
* Capers or thinly sliced caperberries
* Green onions (shallots or white onions are good substitutes)
* Olive oil
* Sea salt flakes
* Freshly ground pepper

###Instructions
Toast the bread. Finely dice the shallots, onions, or green onions.

Liberally butter the toasted bread, and cover the entirety of the toast's surface with a single layer of smoked salmon. Sprinkle the sheet of salmon with lemon juice.

Evenly cover with the diced shallots and capers in about a 3:1 ratio of onions to capers.

Pour a decent amount (about 1 tablespoon) of olive oil onto the sandwich. Scatter sea salt flakes and finish it with some ground pepper.

This is easiest to eat with knife and fork due to the olive oil overdose. Serve with a glass of wine or whole milk, depending on time of day.

[^1]: Aim to buy fresh 'chunk' sushi-grade smoked salmon (ideally, never frozen) from your local fishmonger. Slice it with a sharp knife to your preferred thickness. This allows you to achiveve thinner slices than the pre-sliced smoked salmon from normal grocery stores.

&lt;figure&gt;
	&lt;img src=&quot;/images/smoked-salmon-9.jpg&quot;&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/smoked-salmon-1.jpg&quot;&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/smoked-salmon-7.jpg&quot;&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/smoked-salmon-8.jpg&quot;&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/smoked-salmon-15.jpg&quot;&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/smoked-salmon-16.jpg&quot;&gt;&lt;/a&gt;
&lt;/figure&gt;

*Thank you [Mike King](http://michaelelliotking.com/) for the food photography, Joel Ryan for coining 'openfacers', and [Frank Megna](http://cargocollective.com/frankmegna) for the slogan!* This post originally appeared at openfacers.com, the now-defunct food blog focused solely on open-faced sandwiches.




  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/smoked-salmon-open-faced-sandwich.md/&quot;&gt;Smoked salmon openfacer&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on July 19, 2013.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Reality distortion]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/photography/" />
  <id>http://localhost:4000/photography</id>
  <published>2011-01-01T00:00:00+01:00</published>
  <updated>2011-01-01T00:00:00+01:00</updated>
  <author>
    <name></name>
    <uri>http://localhost:4000</uri>
    <email></email>
  </author>
  <content type="html">&lt;p&gt;I took a bunch of pictures and edited them to be cooler than reality. On hiatus to focus on research, music, and people.&lt;/p&gt;

&lt;p&gt;Because I release everything uncopyrighted, my photos are &lt;em&gt;featured&lt;/em&gt; on &lt;a href=&quot;https://thenewtropic.com/tourism-economy-culture/&quot;&gt;extremely&lt;/a&gt; &lt;a href=&quot;http://viagemempauta.com.br/2015/11/09/destinos-vistos-do-alto/&quot;&gt;random&lt;/a&gt; &lt;a href=&quot;http://www.liligo.co.uk/travel-magazine/a-fall-in-airfare-has-prompted-people-to-travel-more-this-year-21250.html&quot;&gt;websites&lt;/a&gt; (including &lt;a href=&quot;http://www.theatlantic.com/business/archive/2016/05/how-a-neighborhood-block-can-affect-a-persons-success/483983/&quot;&gt;The Atlantic&lt;/a&gt;, &lt;a href=&quot;http://www.huffingtonpost.ca/2015/03/09/loblaw-plans-on-opening-5_n_6829712.html&quot;&gt;HuffPo&lt;/a&gt;, &lt;a href=&quot;https://matadornetwork.com/notebook/11-moments-youll-always-remember-first-cross-country-road-trip/&quot;&gt;Matador&lt;/a&gt;, &lt;a href=&quot;http://www.outsideonline.com/2060641/our-reliance-technology-makes-backcountry-more-dangerous&quot;&gt;Outside Magazine&lt;/a&gt; and &lt;a href=&quot;http://gizmodo.com/preserving-land-isnt-enough-to-save-the-tropics-1770276264&quot;&gt;Gizmodo&lt;/a&gt;!).&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/photography/&quot;&gt;Reality distortion&lt;/a&gt; was originally published by  at &lt;a href=&quot;http://localhost:4000&quot;&gt;PGM-Lab&lt;/a&gt; on January 01, 2011.&lt;/p&gt;</content>
</entry>

</feed>
